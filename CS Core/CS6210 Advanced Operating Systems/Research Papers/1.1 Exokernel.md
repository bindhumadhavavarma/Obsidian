The writer argues most monolithic OS are slow and not flexible since they implment all the low level interfaces inside kernel and user level applications are not allowed to change them and have to use them as they are. The author says he has implemented exokernels where most common system calls are atleast 40 times faster than the Ultrix operating system and is also faster than most state of the art operating systems. he says exception dispatching and interprocess communication are faster in exokernel than in ultrix.

Most Operating systems hide the underlying hardware systems and provide high level abstractions such as file systems, address spaces and interprocess communication. This is wrong for three reasons :
- it denies applications the ad-
vantages of domain-specific optimizations,
- optimizations, it discourages changes

to the implementations of existing abstractions, and it restricts the
flexibility of application builders, since new abstractions can only
be added by awkward emulation on top of existing ones(if they can
be added at all).



The author describes exokernel as In this architecture, a minimal kernel‚Äîwhich we call an exokernel‚Äîsecurely multiplexes available hardware resources. What does multiplexing mean?

To provide an interface that is as low level as possible an exokernel designer has a single overriding goal; to separate protection from management.
To achieve this we can give each application its own virtual machine, but emulating hardware resources will take too much overhead. so instead of emulating we an exokernel exports hardware resources. An exokernel exports using three techniques : 
- Secure binding : applications can securely bind to machine resources and handle events.
- Visible resource revocations : applications can participate in a resource revocation protocol.
- Abort protocol : an exokernel can break secure bindings of uncooperative applications by force.


As in microkernel systems, an exokernel can provide backward
compatibility in three ways: one, binary emulation of the operating

system and its programs; two, by implementing its hardware ab-
straction layer on top of an exokernel; and three, re-implementing

the operating system‚Äôs abstractions on top of an exokernel.

# exokernel design

The challenge for an exokernel is to give library operating systems
maximum freedom in managing physicalresourceswhile protecting

them from each other; a programming error in one library operat-
ing system should not affect another library operating system. To

achieve this goal, an exokernel separates protection from manage-
ment through a low-level interface.

# design principles

- securely expose hardware.
- Expose allocation : the application should be able to request any hardware resouce. no resource should be implicitly allocated, the application should participate in every allocation that takes place.
- Expose names : expose physical names instead of vitual ones like normal OS do.
- expose revocation : An exokernel should utilize a visible re-
source revocation protocol so that well-behaved library operating

systems can perform effective application-level resource manage-
ment. Visible revocation allows physical names to be used easily

and permits library operating systems to choose which instance of
a specific resource to relinquish.

---

# üìùAn Operating System Architecture for Application-Level Resource Management*

**Authors:** Dawson R. Engler, M. Frans Kaashoek, James O‚ÄôToole Jr. (MIT LCS, 1995)

---

## 1. Introduction

### Traditional OS Model

* **Kernels like Unix, Mach, Windows NT** define and enforce **fixed abstractions**:

  * Processes
  * Virtual memory
  * File system
  * IPC (pipes, sockets, RPC)
* These abstractions are *hardwired into the kernel*.
* The kernel decides **both protection** (ensuring apps can‚Äôt interfere) and **management** (deciding policies like paging, scheduling, caching).

### Problem

* Different applications have widely varying needs.

  * Example:

    * A **database** prefers to manage its own buffer cache and replacement policies.
    * A **scientific computation** may want contiguous CPU slices to avoid context switch costs.
    * A **web server** may prefer custom networking stacks or optimized caching.
* Fixed abstractions = **poor performance & lack of flexibility**.

  * Applications can‚Äôt override or customize kernel policies.
  * Performance suffers due to mismatch.

### Exokernel Approach

* Proposes **‚Äúexokernel‚Äù**:

  * Kernel only performs **secure multiplexing** of hardware resources.
  * It enforces *who owns what*, but not *how resources are used*.
* All higher-level abstractions (process, VM, IPC, FS, networking) are implemented in **user-level libraries** called **Library Operating Systems (libOS)**.
* Applications link to the libOS of their choice.

### Benefits

1. **Performance:**

   * Fewer kernel crossings, less overhead.
   * Primitives (syscalls, IPC, exceptions) can be **10‚Äì100√ó faster** than Unix (Ultrix).
   * Applications build abstractions optimized for themselves.

2. **Flexibility:**

   * Applications can choose their own libOS.
   * Abstractions become **replaceable and extensible**.
   * Example: one libOS could export a Unix-like interface, another could export a distributed shared memory interface.

3. **Innovation:**

   * New abstractions can be tested at user level without modifying the kernel.
   * Removes kernel bottleneck for adopting new OS research.

---

## 2. The Case for Exokernels

### 2.1 Problems with Fixed Abstractions

1. **Over-generalization:**

   * Kernels use one-size-fits-all policies (e.g., LRU for paging, global CPU scheduler).
   * Works badly for specialized apps (databases, real-time apps).

2. **Information Hiding:**

   * Kernels hide important hardware events: interrupts, low-level I/O, exceptions.
   * Apps are forced to go through the kernel‚Äôs abstraction.
   * Makes it hard to implement things like:

     * Custom paging
     * User-level threads
     * Distributed shared memory

3. **Slows Innovation:**

   * Any new idea requires kernel modification ‚Üí risky, hard to deploy.
   * Example: scheduler activations, efficient IPC mechanisms ‚Äî proposed in research but rarely adopted.

### 2.2 End-to-End Principle

* **Networking lesson applied to OS design.**
* The **end-to-end principle**: many functions (error correction, encryption, etc.) are best implemented at the endpoints, not the middle.
* Similarly:

  * **Resource management policies** should be implemented at the applications/libOS level, not in the kernel.
* The kernel should only **protect** resources, leaving semantics to applications.

### 2.3 Structure of an Exokernel

* **Exokernel responsibilities:**

  * Track **ownership** of resources.
  * Enforce **protection** at access points.
  * Handle **revocation** when resources are scarce.

* **LibOS responsibilities:**

  * Implement abstractions: processes, VM, file system, IPC, networking.
  * Decide **policies** for paging, scheduling, caching, etc.

* **Applications:**

  * Link against a libOS of their choice.
  * Can even mix and match different libOS libraries for different tasks.

This design **separates protection (kernel)** from **management (libOS)**.

---

## 3. Design Issues in Exokernels

### Core Challenge

* How to securely multiplex hardware without imposing fixed abstractions.
* The kernel must ensure **mutual distrustful apps** don‚Äôt interfere, but still allow **flexibility**.

### 3.1 Secure Bindings

* **Definition:** A mechanism that decouples *authorization* from *use*.
* At **bind time**: complex authorization is performed once.
* At **access time**: fast, simple checks enforce binding.
* Examples:

  * **TLB entry**: At bind time, kernel checks mapping; at runtime, hardware enforces it.
  * **Packet filters**: Apps install filters in the kernel; kernel runs them quickly on each packet, no need to involve apps on every packet arrival.
* Benefits:

  * **Performance**: expensive checks done once.
  * **Simplicity**: kernel doesn‚Äôt need to know high-level semantics.
  * **Flexibility**: apps define policies.

### 3.2 Visible Revocation

* When resources are scarce, the exokernel asks libOS to **voluntarily give back resources**.
* App chooses *which* resources to release.
* Contrast: in traditional OS, kernel unilaterally decides (invisible revocation).
* Example: a database may prefer to give up a page that is least useful to it, rather than having the kernel pick arbitrarily.

### 3.3 Abort Protocol

* If app refuses to release resources:

  * Exokernel forcibly **reclaims** them.
  * Breaks secure bindings.
  * Updates libOS via repossession vector.
* Guarantee: some resources (like bootstrap pages) can be marked **non-revocable**, ensuring app survival.

---

## 4. Prototype Systems and Methodology

### Implemented Prototypes

* **Aegis (Exokernel):**

  * Minimal kernel exporting CPU, memory, TLB, exceptions, interrupts, and network.
  * Implements secure bindings, visible revocation, abort protocol.

* **ExOS (LibOS):**

  * Library OS built on Aegis.
  * Implements Unix-like abstractions (processes, VM, file system, IPC).
  * Includes TCP/IP stack (IP, UDP, NFS client).
  * Serves as proof that traditional abstractions can be implemented efficiently at user level.

* **Other prototypes:**

  * Glaze (exokernel for multiprocessor SPARC).
  * PhOS (parallel libOS).

### Methodology

* Compare **Aegis/ExOS** to **Ultrix 4.2** (reference Unix).
* Benchmarks:

  * Primitive ops (syscalls, exceptions, IPC, TLB misses).
  * Application-level abstractions (pipes, shared memory, LRPC).
  * Network performance (packet classification).
* Hardware: DECstation machines (MIPS R2000/R3000 CPUs).
* Performance normalized using SPECint.

### Key Hypotheses Tested

1. An exokernel can be implemented with **low overhead**.
2. Secure multiplexing (secure bindings) is feasible and efficient.
3. Traditional abstractions (processes, VM, IPC) can be re-implemented at user level with competitive or superior performance.
4. Applications can build **special-purpose abstractions** impossible under monolithic OSes.

---

## ‚úÖ Section 1‚Äì4 Takeaways

* Traditional OSes lock in abstractions, causing mismatch and inefficiency.
* Exokernel separates **protection (kernel)** from **management (libOS)**.
* Core techniques: **secure bindings**, **visible revocation**, **abort protocol**.
* Prototype **Aegis + ExOS** demonstrates feasibility.
* Early results suggest **huge performance gains** (orders of magnitude faster).
* Most importantly: this architecture encourages **flexibility and innovation** in OS design.


---

# üìù **Aegis: The Exokernel Implementation**

---

## üîπ Overall Picture

Aegis is the first prototype of an **exokernel**. Its purpose is to prove that the exokernel philosophy ‚Äî exposing hardware resources directly to applications while the kernel only enforces protection ‚Äî can actually be implemented **both securely and efficiently**.

The paper explores how Aegis handles critical OS responsibilities:

1. CPU multiplexing (scheduling).
2. Event handling (exceptions, interrupts, control transfers).
3. Virtual memory and TLB management.
4. Inter-process communication.
5. Networking (packet demultiplexing).

Throughout the section, Aegis is compared against Ultrix (a traditional Unix OS). Performance results show that Aegis is consistently **10‚Äì100x faster**, mainly because it strips away high-level abstractions and only enforces **secure bindings**.

---

## üîπ CPU Multiplexing in Aegis

* Aegis models the CPU as a **linear vector of time slices**.

  * Each ‚Äúslot‚Äù in the vector represents a slice of CPU time at clock-tick granularity.
  * Applications can allocate time slices much like allocating pages of memory.

* Scheduling is a simple **round robin** through the time-slice vector.

  * The **position** of a slice is important: it encodes both the order in which the slice will run and an upper bound on when it will be scheduled.
  * This gives applications control to balance **latency** vs **throughput**.

    * Example: a scientific app might take consecutive slices (maximize throughput).
    * An interactive app might take evenly spread slices (maximize responsiveness).

* **Timer interrupts** mark the start/end of each slice.

  * Aegis treats these interrupts like exceptions: the kernel jumps to user-supplied handlers.
  * Applications themselves are responsible for context switching: saving registers, releasing locks, etc.
  * This flexibility means applications can implement custom scheduling policies (e.g., scheduler activations).

* **Fairness**:

  * If an app fails to yield after its slice ends, Aegis increments an **excesstime counter**.
  * The app is penalized by losing future slices; if it continues abusing time, its environment can even be destroyed.
  * This enforces fairness while still leaving scheduling policy in user space.

---

## üîπ Processor Environments

In Aegis, every resource is tied to an **environment**, which acts like a container for the application‚Äôs execution context. An environment includes all the state needed for the kernel to deliver events safely to the application.

Each environment includes **four kinds of contexts**:

1. **Exception Context**

   * Specifies the handler PC and save area for registers when exceptions occur.

2. **Interrupt Context**

   * Holds handler PCs and register save areas for device/timer interrupts.
   * Timer interrupts have distinct handlers for ‚Äúslice start‚Äù and ‚Äúslice end.‚Äù

3. **Protected Entry Context**

   * Defines entry points for protected control transfers (PCTs).
   * Any environment can transfer control to another, with access control enforced at the application level.

4. **Addressing Context**

   * Holds guaranteed mappings, an address-space ID, and a tag for the software TLB.
   * These ensure TLB misses can be handled and that environments can bootstrap their own page tables.

Together, these contexts represent what we normally call a ‚Äúprocess,‚Äù but in exokernel style, they are explicit and minimal.

---

## üîπ System Calls and Performance

* Aegis divides system calls into two categories:

  * **Fast path** (no stack needed).
  * **Slow path** (requires stack).

* Performance is outstanding:

  * On DEC5000, Aegis system calls take **1.6‚Äì2.3 Œºs**.
  * Ultrix system calls take **118‚Äì326 Œºs**.

* Why so much faster?

  * Aegis does not maintain complex kernel data structures (like page tables).
  * It does minimal work ‚Äî just enforcing ownership and protection.
  * In contrast, Ultrix has to carefully manage TLB faults and multiplex high-level abstractions.

---

## üîπ Exception Dispatching

* Unlike traditional OSes, Aegis **delivers hardware exceptions directly to applications** (except for system calls).

* The process:

  1. Save a few scratch registers to a safe memory area.
  2. Load faulting address, cause, and exception PC.
  3. Jump directly to the application‚Äôs registered handler.

* Crucially, this takes **only 18 instructions** (~1.5 Œºs), compared to ~150‚Äì200 Œºs on Ultrix.

* Because apps handle their own exceptions, they can implement things like:

  * Distributed shared memory.
  * Garbage collectors.
  * Persistent object stores.

This is a major demonstration of the exokernel principle: **fast, direct delivery with the kernel out of the way.**

---

## üîπ Virtual Memory

Two big challenges: **bootstrapping** and **efficiency**.

### Bootstrapping

* Problem: what if a TLB miss happens while trying to handle another exception?
* Solution: **guaranteed mappings**.

  * A reserved region of VA space always maps to physical memory.
  * This holds page tables and exception code, ensuring Aegis can always recover.

### Efficiency

* Aegis uses a **Software TLB (STLB)** as a cache for secure bindings.

  * 4096 entries, direct-mapped, in physical memory.
  * Lookup takes ~18 instructions (~1‚Äì2 Œºs).
* On a TLB miss:

  1. Aegis checks STLB. If hit ‚Üí refill TLB immediately.
  2. Otherwise, app handles the miss.

This avoids expensive upcalls on most misses and keeps VM management under application control.

---

## üîπ Protected Control Transfers (IPC substrate)

* **Protected Control Transfer (PCT)** is a primitive for fast IPC.

* Mechanism:

  * Switch PC to callee‚Äôs entry point.
  * Donate current (or future) time slice.
  * Install callee‚Äôs environment state (addressing, registers, etc.).

* Two types:

  * **Asynchronous**: donate only remaining slice.
  * **Synchronous**: donate full slice, including future slices until explicitly returned.

* Guarantees:

  1. **Atomicity** ‚Äî once started, control always reaches the callee.
  2. **Register integrity** ‚Äî no registers clobbered, so registers can serve as message buffers.

* Cost: about **30 instructions (~1.4‚Äì2.9 Œºs)**, much faster than L3‚Äôs RPC mechanism (after normalization, ~6.6x faster).

This shows that even sophisticated IPC can be efficiently built on a minimal substrate.

---

## üîπ Networking

* Traditional packet filters (like BPF) interpret filter programs, which is costly.

* Aegis introduces **Dynamic Packet Filter (DPF)**:

  * Compiles filters into native code at install time.
  * Aggressively optimizes code with constants.
  * Uses **VCODE**, a portable dynamic code generation library.

* Performance comparison (classifying packets for 10 filters on DEC5000/200):

  * MPF: 35 Œºs.
  * PATHFINDER: 19 Œºs.
  * DPF: **1.5 Œºs**.

This is an order of magnitude faster, and again illustrates the **secure binding principle**: the kernel enforces only ownership while apps provide logic in a fast, verifiable form.

---

## üîπ Key Lessons from Aegis

1. **Minimal kernel state** ‚Üí keeping ownership simple allows very efficient implementation.
2. **Small kernel** ‚Üí lean and avoids heavy abstractions; all complex management pushed to applications.
3. **Caching secure bindings** ‚Üí STLB makes VM efficient.
4. **Code downloading** ‚Üí packet filters show how dynamic code gen avoids costly context switches.
5. Across the board, Aegis beats Ultrix by wide margins:

   * Syscalls, exceptions, TLB misses, IPC, and networking are all 10‚Äì100x faster.

---

## ‚úÖ Final Takeaway

Aegis proves that the exokernel idea isn‚Äôt just theoretical ‚Äî a kernel that only enforces low-level protection and multiplexing can outperform traditional monolithic kernels by a huge margin, while still giving applications full flexibility to build their own abstractions in user space.

---

# *ExOS: Application-Level Operating System Abstractions*

---

## üîπ Big Idea of ExOS

* **Unusual aspect**: ExOS implements **basic OS abstractions (VM, processes, IPC, networking)** entirely at **application level**, inside the app‚Äôs own address space.
* Goal: Show that even fundamental OS services can be **moved out of the kernel** and implemented efficiently in untrusted libraries.
* Focus in this section:

  1. **IPC (Inter-Process Communication)**
  2. **Virtual Memory (VM)**
  3. **Remote communication (networking)**

---

## 1. Inter-Process Communication (IPC) in ExOS

### Background

* **Fast IPC** is crucial for modular, decoupled systems.
* Aegis provides **Protected Control Transfers (PCTs)** as a primitive (see Section 5 in paper).
* ExOS builds higher-level IPC abstractions on top of these.

### Experiments (Table 8 in paper)

Three IPC mechanisms tested with ping-pong style benchmarks:

1. **Pipes**

   * Ultrix: Uses standard Unix kernel-managed pipes.
   * ExOS: Pipes implemented as **shared-memory circular buffers**.

     * Writing to a full buffer or reading from an empty one ‚Üí process yields its slice to peer (using Aegis yield).
     * Two implementations:

       * `pipe`: basic version.
       * `pipe‚Äô`: optimized version, with inline read/write calls (possible since library is in user space).
   * **Result**: Even unoptimized ExOS pipes are ~10√ó faster than Ultrix pipes.

2. **Shared Memory (shm)**

   * Simple shared counter ping-pong.
   * ExOS: Uses `yield` for synchronization.
   * Ultrix: No `yield`, so synthesized with **signals** (much more expensive).
   * **Result**: ExOS shm is 15‚Äì20√ó faster than Ultrix shm, and about 2√ó faster than ExOS pipes.

3. **Lightweight RPC (lrpc)**

   * ExOS: Built directly on **protected control transfers**.

     * Saves callee-saved registers.
     * Assumes one function of interest (no lookup tables, no permission checks).
     * Single-threaded.
   * Ultrix: No support for lrpc. Must emulate using:

     * **Pipes** (46‚Äì60√ó slower than ExOS).
     * **Signals** (26‚Äì37√ó slower).
   * **Result**: ExOS makes efficient lrpc possible; Ultrix either pays huge overhead or requires kernel modification.

### Takeaway

* By exposing low-level primitives (PCT, yield) and moving abstractions into libraries, ExOS builds **much faster IPC mechanisms** than Ultrix.
* Shows that **user-level IPC abstractions can outperform kernel-based IPC**.

---

## 2. Virtual Memory in ExOS

### Implementation

* ExOS VM system is small (~1000 lines of code).

* **Limitations:**

  * No swapping.
  * Page tables are implemented as a **linear vector** (O(log n) lookup via binary search).

* **Features:**

  * Aliasing and sharing.
  * Per-page cache control (enable/disable).
  * Precise page allocation.
  * DMA support.

### Benchmarks

1. **Matrix Multiply Test (Table 9)**

   * 150√ó150 integer matrix multiply.
   * No special optimizations (e.g., page coloring).
   * **Result**: ExOS ‚âà Ultrix in performance.
   * Shows: moving VM to application level doesn‚Äôt add overhead for typical workloads.

2. **Appel & Li VM Benchmarks (Table 10)**

   * Measure critical VM ops for DSM and garbage collectors.

   * Operations tested:

     * `dirty`: query if a page is dirty.
     * `prot1`: protect a single page.
     * `prot100`: protect 100 pages.
     * `unprot100`: unprotect 100 pages.
     * `trap`: handle a page-protection trap.
     * `appel1`: access a protected page; in handler protect another + unprotect current.
     * `appel2`: protect 100 pages; touch each in random order; handler unprotects current.

   * **Observations:**

     * ExOS supports **dirty** queries (Ultrix doesn‚Äôt). Shows flexibility.
     * `dirty` is cheap (just parse ExOS‚Äôs page table).
     * `prot1`: ExOS ~2√ó faster than Ultrix (due to faster syscalls, lightweight kernel).
     * `prot100`/`unprot100`: Ultrix is 1.1‚Äì1.6√ó faster for bulk contiguous protection (because ExOS accesses two structures: STLB + page table).
     * `trap`: ExOS **10‚Äì30√ó faster** than Ultrix (faster exception dispatch).
     * `appel1`/`appel2`: ExOS ~10√ó faster overall, despite slower bulk ops.

   * **Reasoning:**

     * ExOS benefits from **fast exception delivery** and lightweight syscalls.
     * Further improvements possible with better data structures (hash tables) and hand-tuned assembly.

### Takeaway

* ExOS demonstrates that **application-level VM management is feasible and efficient**.
* Some bulk ops are slower due to naive implementation, but performance on complex workloads is much better than Ultrix.

---

## 3. Networking and Remote Communication

### Motivation

* Networking is latency-sensitive.
* Some operations can‚Äôt be safely done in app space (e.g., mapping NIC buffers securely).
* ExOS therefore allows **downloading application-specific handlers (ASHs)** into the kernel.

### Application-Specific Handlers (ASHs)

* **Definition**: Untrusted message handlers provided by apps, downloaded into kernel.

* Made safe by **code inspection** and **sandboxing**.

* Run immediately on message arrival, even if app isn‚Äôt scheduled.

* **Why useful?**

  * Normal kernel crossing cost is low in Aegis (18 instructions), but network buffer mapping is tricky.
  * ASHs can run during message arrival to process data early.

### Abilities of ASHs

1. **Direct message vectoring**: Control exactly where incoming data is copied ‚Üí avoids intermediate copies.
2. **Dynamic Integrated Layer Processing (ILP):**

   * Combine data transforms (e.g., checksumming, conversion) directly into transfer engine.
   * ASHs treat data as ‚Äúpipes‚Äù ‚Äî modular units that can be dynamically composed.
   * Enables runtime-assembled protocol pipelines.
3. **Message initiation**: ASHs can send replies immediately (low-latency).
4. **Control initiation**: ASHs can perform computation and control ops (e.g., active messages, remote lock acquisition).

### Performance (Table 11 & Fig. 2)

* **Roundtrip latency (60B UDP packet):**

  * Ultrix: 3400 Œºs.
  * Ultrix FRPC (optimized RPC): 340 Œºs.
  * ExOS (no ASH): 320 Œºs.
  * ExOS (with ASH): 259 Œºs.

* **Result:**

  * ExOS/ASH beats Ultrix FRPC despite running on slower hardware.
  * Only ~6 Œºs slower than **theoretical Ethernet lower bound**.

* **Scalability with many processes (Fig. 2):**

  * With ASHs: latency stays constant (decoupled from scheduling).
  * Without ASHs: latency rises linearly with number of processes (must wait for app scheduling).
  * Ultrix: even worse and erratic (0.5‚Äì4.5 ms with 10 procs).

### Takeaway

* ASHs demonstrate that **safe, downloaded handlers** can provide near-optimal network performance.
* They **decouple message handling from process scheduling**, which is critical for latency-sensitive ops.
* Networking performance is another proof that **application-level OS abstractions are viable**.

---

## ‚úÖ Section Summary (IPC, VM, Networking in ExOS)

1. **IPC:**

   * Pipes, shm, lrpc all much faster than Ultrix equivalents.
   * Emulation of new primitives on Ultrix is very costly.
2. **Virtual Memory:**

   * Application-level VM management is efficient.
   * ExOS outperforms Ultrix in most critical benchmarks (especially traps, composite ops).
   * Flexibility to add features like ‚Äúdirty bit queries.‚Äù
3. **Networking (ASHs):**

   * Downloaded handlers enable near-hardware latency performance.
   * Decouple message handling from scheduling ‚Üí stable low latency even under load.

üëâ Overall, ExOS proves that **core OS abstractions (IPC, VM, networking) can be moved entirely to application space without loss of performance, and often with significant gains.**

---

