
> makenotes

# ESX Server (Waldspurger OSDIâ€™02) â€” Memory Resource Management (Notes)

## ğŸ§­ Context & Goals

* **ESX Server**: thin VMM that **multiplexes hardware** directly (bare-metal), runs **unmodified OSes** (e.g., Windows 2000 AS, Red Hat 7.2).
* Goal: **efficient overcommit** of memory while preserving **isolation & guarantees** (statistical multiplexing without DoS).
* Differs from **VMware Workstation** (hosted): ESX bypasses host OS for I/O â†’ **higher I/O performance** + full **resource control**.

## ğŸ§ª Core Contributions (Memory)

1. **Ballooning**

   * Reclaims memory **from inside the guest** by inflating a driver that allocates guest pages â†’ guest hands back **least-valuable** pages (guest-informed reclamation).

2. **Idle Memory Tax**

   * Policy that **penalizes idle pages** to achieve high utilization while maintaining **performance isolation** (discourages hoarding).

3. **Content-Based Page Sharing (CBPS)**

   * Detects **identical page contents across VMs**, remaps to a **single machine page** (copy-on-write on write) â†’ reduces footprint **transparently**.

4. **Hot I/O Page Remapping**

   * Remaps frequently copied I/O pages to **avoid extra copies** on large-memory systems (reduces I/O copy overhead).

â¡ï¸ **Combined effect**: safely support **memory overcommit** with low overhead.

---

## ğŸ§± Memory Virtualization Model (Section 2)

### Terminology

* **Machine address (MPN)**: real hardware page frame.
* **â€œPhysicalâ€ address (PPN)**: **virtualized** guest-physical frame the VM believes it owns (quoted â€œphysicalâ€ in paper to emphasize abstraction).

### Two-Level Translation

* ESX maintains per-VM **pmap**: **PPN â†’ MPN** mapping.
* Hardware uses **shadow page tables** containing **virtual â†’ machine** translations (Vâ†’M), synchronized with pmap (PPNâ†’MPN).
* Guest OS attempts to modify its own page tables / TLB are **intercepted**; ESX updates **shadow** accordingly.

### Why shadowing?

* Allows **unmodified OSes**: hardware sees valid page tables (Vâ†’M); guest still believes it manages PPNs.
* **TLB** caches Vâ†’M directly from shadow tables â†’ **ordinary memory refs incur no extra cost** post-translation (fast steady-state).

### Power of Extra Indirection

* ESX can **transparently remap** a guest â€œphysicalâ€ page by changing **PPNâ†’MPN** in pmap.
* Enables:

  * **Balloon reclamation** (reassign MPNs elsewhere),
  * **Page sharing** (multiple PPNs â†’ same MPN, with CoW),
  * **Monitoring/interposition** on guest memory accesses,
  * **Hot I/O remap** optimizations.

---

## ğŸ”§ Mechanisms vs. Policies (paper roadmap)

* **Low-level mechanisms**: memory virtualization (shadowing), ballooning, sharing, hot I/O remap.
* **Policies**: proportional-share with **working set estimates**, **idle memory tax**, high-level **target allocations** per VM.
* Sections:

  * Â§2 Memory virtualization (pmap, shadows, traps),
  * Â§3 Reclamation (ballooning, others),
  * Â§4 Sharing (CBPS),
  * Â§5 Working-set in proportional-share,
  * Â§6 High-level allocation policy,
  * Â§7 Hot I/O page remapping,
  * Â§8 Related work, Â§9 Conclusions.

---

## ğŸ“ Quick Compare: ESX vs. Xen (for intuition)

* **ESX (full virt)**: **shadow page tables** (Vâ†’M) + pmap (PPNâ†’MPN); runs **unmodified** guests.
* **Xen (paravirt)**: guests manage **HW-visible page tables** but **read-only**; updates via **hypercalls** (no shadowing), needs **guest mods**.
* Trade-off: **compatibility (ESX)** vs **lower MMU overhead (Xen paravirt)**.

---

## ğŸ”‘ Takeaways

* ESXâ€™s memory layer hinges on **an extra PPNâ†’MPN indirection** + **shadow PTEs** to keep CPU fast while enabling rich policies.
* **Ballooning + idle tax + sharing + hot remap** = robust toolkit to **overcommit safely** with isolation and good performance for unmodified OSes.
---

> makenotes

# VMware ESX Server â€” Section 3: Reclamation Mechanisms

## ğŸ§© Overview: Memory Overcommitment

* **Overcommitment**: total configured VM memory > available physical machine memory.
* Enables **higher server consolidation** than static partitioning.
* Each VM sees a **fixed â€œphysicalâ€ size** (max allocation) post-boot â†’ guest OSes assume static memory.
* ESX automatically **allocates or reclaims** memory per VM based on configuration + load.

---

## âš ï¸ 3.1 Page Replacement Issues

### Traditional Approach

* Older VMMs (e.g., Disco, VM/370) added a **second-level paging**:
  VM â€œphysicalâ€ pages are **swapped to disk** by the VMM.
* Requires **meta-level replacement policy** â†’ decides:

  1. Which VM to reclaim from.
  2. Which guest page within that VM to swap.

### Problems

1. **Limited insight**: Only the guest OS knows which pages are least valuable.

   * Each OS has its own undocumented, version-specific policies.
2. **Performance anomalies**: Meta-policy may conflict with guest paging decisions.
3. **Double paging**: both levels may evict the same page, causing:

   * VM swap-out â†’ guest swap-out â†’ redundant disk I/O (page pulled in, immediately swapped again).

---

## ğŸˆ 3.2 Ballooning

### Concept

* **Balloon driver** inside guest OS acts as a mediator for memory reclamation.
* ESX inflates/deflates this balloon to adjust guestâ€™s memory pressure.

### Mechanism

1. **Inflate** â†’ balloon allocates pinned pages using normal OS APIs.
   â†’ Guest OS invokes its **own replacement policy**, deciding what to evict.
2. **Deflate** â†’ balloon frees previously allocated pages.
3. **ESX reclaims corresponding machine pages** once the balloon holds them.

> âœ… **Effect**: VM behaves as if configured with less memory â€” predictable, guest-aware reclamation.

### Implementation Details

* Guest balloon module = pseudo-device driver with **no user interface**.
* Communicates with ESX via **private control channel**.
* Polls ESX once per second for target size; adapts allocation rate to avoid stress.
* Typical APIs used:

  * **Linux** â†’ `get_free_page()`
  * **Windows** â†’ `MmAllocatePagesForMdl()`, `MmProbeAndLockPages()`
* **Fault handling**:

  * ESX annotates ballooned PPNs â†’ deallocates MPNs.
  * If guest touches reclaimed PPN â†’ trap handled by ESX (â€œpopsâ€ balloon, allocates new MPN).
  * Newly allocated pages are **zeroed** (prevent info leak) + **cache-color aware**.

### Performance

* Experiment: `dbench` (40 clients, dual-CPU Dell Precision 420, Red Hat 7.2).
* Throughput nearly identical to non-ballooned runs:

  * Overhead: **4.4% at 128 MB**, **1.4% at 224 MB**.
* Slight loss due to kernel data structures scaled by total â€œphysicalâ€ memory (more overhead in larger VM).

### Limitations

* Balloon driver may be:

  * Unavailable (boot phase), disabled, uninstalled.
  * Too slow to reclaim under heavy pressure.
  * Bound by guest OS constraints (max alloc sizes, pinned memory rules).
* Future possibility: **hot-pluggable virtual memory cards** â†’ coarse-grained ballooning.

---

## ğŸ§  3.3 Demand Paging

### When Used

* **Fallback** when ballooning is insufficient or unavailable.

### Mechanism

* ESX swap daemon:

  * Receives per-VM **target swap levels** from policy layer.
  * Selects candidate pages + performs **asynchronous page-outs**.
  * Uses **clustering** and **free-slot caching** for I/O efficiency.
* Pages swapped from **machine memory to ESX swap file** (no guest involvement).

### Policy

* **Randomized page replacement**:

  * Avoids destructive interference with guest paging algorithms.
  * Acceptable since paging is rare (last resort).
* Future work: **adaptive or per-VM policies** for more refined control.

---

## ğŸ§¾ Summary Table

| Mechanism                  | Who Decides Evictions       | Guest Involvement | Advantages                                   | Drawbacks                                            |
| -------------------------- | --------------------------- | ----------------- | -------------------------------------------- | ---------------------------------------------------- |
| **Ballooning**             | Guest OS (native algorithm) | Yes               | Predictable perf, guest-aware                | Requires balloon driver                              |
| **Demand Paging**          | ESX meta-policy             | No                | Works even if guest unmodified/uncooperative | Limited insight, potential double paging if frequent |
| **Randomized Replacement** | ESX (fallback)              | No                | Avoids interference                          | Less optimal reuse decisions                         |

---

### ğŸ§© Key Takeaway

> ESX Server reclaims memory **intelligently** using **ballooning** as the cooperative first line and **paging** as the safety net â€” combining guest-aware decisions with hypervisor-level control for efficient and safe overcommitment.

---
> makenotes

# VMware ESX Server â€” Section 4: Sharing Memory

## ğŸ§  Overview

Server consolidation naturally creates **memory redundancy** across VMs â€” e.g., same OS kernels, applications, or data pages loaded in multiple instances.
**ESX Server** exploits this by transparently identifying identical pages and allowing multiple VMs to share the same machine page.
This **reduces total memory footprint**, allowing **higher overcommitment** while maintaining performance.

---

## ğŸ” 4.1 Transparent Page Sharing (Background â€“ from Disco)

* First introduced by **Disco** (Stanford VMM project) to eliminate redundant copies across VMs.
* **Mechanism**: identical pages â†’ mapped to a **single machine page**, marked **copy-on-write (COW)**.

  * Write â†’ page fault â†’ private copy created.
* **Limitation (Disco)**:

  * Required **guest OS modifications** to identify and hook into page-creation routines (e.g., `bcopy()` for buffer sharing).
  * Relied on **non-standard interfaces** (e.g., large-packet virtual network interfaces, shared nonpersistent disks).
  * Not feasible in ESXâ€™s environment where **unmodified OSes must run**.

---

## ğŸ§© 4.2 Content-Based Page Sharing (ESXâ€™s Approach)

### Core Idea

* Identify identical pages **purely by their contents**, not by how/where they were created.
* Uses **content hashing** rather than OS-level hooks.
* Advantages:

  * No need to modify or understand guest OS or APIs.
  * Works across all workloads, regardless of application behavior.

### Algorithm

1. **Scan candidate pages** and compute a **64-bit hash** for each.
2. Use the hash as a lookup key in a **global hash table** of already shared (COW) pages.
3. If a hash match is found â†’ perform **byte-by-byte comparison** to confirm.

   * If identical â†’ update guestâ€™s mapping: multiple PPNs â†’ one shared MPN (marked COW).
   * If not identical â†’ page remains unshared (stored as a *hint* for future matches).

> âœ… **Optimization:**
> Pages not matched are not immediately marked COW â€” instead stored as *hints*.
>
> * On rehash, if content changed â†’ hint discarded.
> * If unchanged â†’ full compare, then shared.

### Scanning Policy

* **Random or incremental scanning** of pages (configurable rate).
* Can prioritize:

  * Pages marked **read-only**.
  * Pages containing **executable code**.
* To limit overhead, scans often occur **during idle CPU cycles**.

---

## âš™ï¸ 4.3 Implementation Details

### Data Structures

* **Global hash table** (handles all scanned pages):

  * Entries (frames) = 16 bytes each.
  * Two frame types:

    * **Shared frame:** stores full hash, MPN, ref count, and chain link.
    * **Hint frame:** stores truncated hash + back-reference (VM ID + PPN).
* **Reference counting:**

  * Each shared page tracks number of VMs referencing it.
  * Overflow handled in a secondary table (for heavily shared pages, e.g., all-zero pages).
* **Hashing:**

  * Fast, high-quality 64-bit hash function used.
  * Probability of collision in 64 GB memory < 0.01%.
* **Overhead:**

  * Total memory overhead < **0.5% of system memory**.

### Optimizations

* Share-before-swap: ESX tries to **share a page before paging it out**.
* Random scanning keeps CPU overhead negligible.

---

## ğŸ“Š 4.4 Experimental Results

### ğŸ§ª Controlled â€œBest Caseâ€ â€” Homogeneous VMs

* Setup: 1â€“10 identical Linux VMs (40 MB each) running SPEC95 for 30 mins.
* Hardware: Dell PowerEdge 1400SC, dual 933 MHz Pentium III CPUs.

#### Results (Figure 4)

| Metric         | Observation                                                      |
| -------------- | ---------------------------------------------------------------- |
| Single VM      | 5 MB reclaimed (55% = zero pages)                                |
| Increasing VMs | Sharing grows **linearly** with VM count                         |
| With 10 VMs    | **67% of VM memory shared**, **~60% reclaimed**                  |
| Overhead       | Negligible (â‰¤ 0.5% difference in CPU-bound benchmark throughput) |

> ğŸ” Most sharing from **redundant code and read-only data**, not zero pages.

### ğŸ­ Real-World Deployments (Figure 5)

| Workload | VMs | OS         | Shared MB (%) | Reclaimed MB (%) | Notes                      |
| -------- | --- | ---------- | ------------- | ---------------- | -------------------------- |
| **A**    | 10  | Windows NT | 880 (42.9%)   | 673 (32.9%)      | Mixed DB/web/dev workloads |
| **B**    | 9   | Linux      | 539 (29.2%)   | 345 (18.7%)      | Mail + web servers         |
| **C**    | 5   | Linux      | 165 (10%)     | 120 (7.2%)       | Web proxy, mail, SSH       |

* **Zero pages** contributed a portion (e.g., 70 MB in workload B).
* CPU overhead: negligible to slightly positive (improved cache locality).

---

## ğŸ§¾ Summary Table

| Aspect                   | Disco                   | ESX Server                            |
| ------------------------ | ----------------------- | ------------------------------------- |
| Requires OS modification | âœ… Yes                   | âŒ No                                  |
| Identification method    | Hooked routines         | Content-based hashing                 |
| Type of pages shared     | OS + buffer cache pages | Any identical content                 |
| Copy-on-write            | Yes                     | Yes                                   |
| Overhead                 | Moderate                | < 0.5%                                |
| Typical savings          | N/A                     | 7â€“67% of memory depending on workload |

---

## ğŸ§© Key Takeaways

* **Content-based page sharing** is a cornerstone of ESXâ€™s efficiency: it enables **automatic deduplication** of identical pages across unmodified VMs.
* Works seamlessly without requiring any cooperation from the guest OS or applications.
* Real-world results confirm **substantial memory savings** with minimal CPU cost, making high levels of **safe overcommitment** practical in production environments.
---

> makenotes

# VMware ESX Server â€“ Section 5: Shares vs Working Sets

## ğŸ¯ Goal

Balance **memory efficiency** with **performance isolation** across virtual machines (VMs).
Traditional OSes optimize global throughput, but ESX Server introduces **share-based memory management** and an **idle-memory tax** to enforce fairness while improving utilization.

---

## âš–ï¸ 5.1 Share-Based Allocation

* **Shares** represent proportional rights to memory.

  * Each VM receives memory proportional to its share count.
  * If total demand exceeds supply, allocations scale down proportionally.
  * Over-provisioned VMs release space to those with higher *shares per page* (a â€œpriceâ€ metric).
* **Min-funding revocation algorithm:**

  * Chooses a â€œvictimâ€ VM with the fewest shares per allocated page.
  * Memory is â€œsoldâ€ from low-priority (low-price) VMs to higher-priority ones.
* Graceful degradation â†’ resource rights degrade smoothly under load, guaranteeing minimum fractions while allowing dynamic growth.

---

## ğŸ’¤ 5.2 Reclaiming Idle Memory

**Problem:** Pure proportional allocation wastes spaceâ€”idle high-share VMs hoard memory while active low-share VMs starve.

**Solution:** ESX introduces an **Idle Memory Tax (IMT)**

* **Idea:** Charge more for pages that arenâ€™t actively used.

* When memory is scarce â†’ reclaim from idle VMs first.

* **Tax Rate (Ï„):**

  * Controls aggressiveness of reclamation.
  * Ï„ = 0 â†’ pure share-based (no reclaim).
  * Ï„ = 1 â†’ reclaim all idle pages.

* Default Ï„ = 75 % â€” balances responsiveness with buffer for sudden workload growth.

* Adjusted shares-per-page ratio Î±:

  [
  Î± = \frac{shares}{(active pages + (1 âˆ’ Ï„) Ã— idle pages)}
  ]

* Effect â†’ idle memory gradually reclaimed; active VMs retain priority; policy easily tunable.

---

## ğŸ“Š 5.3 Measuring Idle Memory (Working Set Estimation)

**Goal:** Estimate fraction of each VMâ€™s memory thatâ€™s actively used â€” without guest OS cooperation.

### Implementation

* **Sampling** approach (statistical, not intrusive):

  1. Periodically select â‰ˆ 100 guest pages every 30 s of VM runtime.
  2. Invalidate their mappings â†’ next access triggers a minor fault.
  3. Count â€œtouchedâ€ pages = active subset estimate (â‰ˆ touched / sampled).
* **Advantages:**

  * Works across any OS (no special APIs).
  * Ignores DMA issues that bypass page-table bits.
  * Overhead negligible (< 100 minor faults / 30 s).

### Smoothing and Responsiveness

* Maintains **three moving averages** of activity:

  * **Fast** â†’ tracks rapid spikes.
  * **Slow** â†’ provides stability.
  * **Current-period fast** â†’ captures in-progress surges.
* Final estimate = **max(fast, slow, current)**
  â†’ fast response to increased activity; gradual decay for inactivity.

---

## ğŸ§ª 5.4 Experimental Results

### Memory Sampling (Windows VM)

* Windows 2000 VM with a â€œmemory toucherâ€ app shows ESX accurately tracking working set size.
* Fast average tracks growth; slow average lags during decrease â†’ desired hysteresis.
* Unexpected spike after workload ends = Windows **zero page thread** zeroing freed pages.

### Idle Memory Tax Effectiveness

* Setup: Two VMs (each 256 MB, equal shares) on 512 MB server.

  * VM1 = idle Windows 2000.
  * VM2 = active Linux (dbench workload).
* At Ï„ = 0 % (pure shares): both get â‰ˆ 179 MB allocation â†’ inefficient.
* Raise Ï„ â†’ 75 % â†’ idle VM loses memory, active VM gains memory â†’ throughput â†‘ by 30 %.

---

## ğŸ§­ Summary

| Mechanism              | Purpose                    | Key Idea                                 | Outcome                  |
| ---------------------- | -------------------------- | ---------------------------------------- | ------------------------ |
| **Shares**             | Guarantee fairness         | Allocate memory proportional to weights  | Predictable isolation    |
| **Idle Memory Tax**    | Boost utilization          | Penalize idle pages, reclaim them first  | Active VMs get priority  |
| **Sampling**           | Estimate usage             | Randomly invalidate pages â†’ track access | OS-independent accuracy  |
| **Adaptive Averaging** | Stability + Responsiveness | Combine fast & slow EWMA                 | Smooth allocation shifts |

**Result:** ESX Server achieves efficient, fair, and adaptive memory distribution across VMs â€” maintaining QoS isolation while maximizing hardware utilization.

---

> makenotes

# VMware ESX Server â€“ Section 6: Allocation Policies

## âš™ï¸ Overview

ESX Server dynamically manages VM memory through a coordinated system of **share-based entitlements**, **working-set estimates**, and **reclamation mechanisms** (ballooning + paging + page-sharing).
The goal is to keep host memory utilization high while upholding isolation and fairness guarantees.

---

## ğŸ§© 6.1 Parameters â€” Administrator Controls

Each VM is configured with three key parameters:

| Parameter    | Meaning                      | Guarantee                             |
| ------------ | ---------------------------- | ------------------------------------- |
| **min size** | Guaranteed lower bound       | Always allocated, even under pressure |
| **max size** | Configured â€œphysicalâ€ memory | Upper limit visible to guest OS       |
| **shares**   | Relative entitlement weight  | Determines proportional allocation    |

* When memory is plentiful â†’ each VM gets its **max size**.
* When overcommitted â†’ allocations shrink toward **min**, proportional to shares and activity.
* Two VMs with 2:1 share ratio get approximately 2Ã— and 1Ã— memory respectively, subject to their bounds.

---

## ğŸ›¡ï¸ 6.2 Admission Control

Before power-on, ESX verifies sufficient resources:

* **Machine memory reservation = min + overhead** (per-VM metadata + graphics frame buffer + pmap/shadow tables).

  * Typical overhead â‰ˆ 32 MB (4â€“8 MB for frame buffer).
* **Swap space reservation = max âˆ’ min** â†’ guarantees VM state preservation if all memory must be paged.
* Only a small fraction is usually used at runtime.
* Reservations control admission but do not statically tie up memory â€” unused space can be reallocated.

---

## ğŸ”„ 6.3 Dynamic Reallocation

ESX continuously adjusts allocations based on:

* Parameter changes (admin edits, VM start/stop).
* Free-memory threshold crossings.
* Periodic updates from idle-memory estimates.

### Reclamation Thresholds (Defaults in % of system RAM)

| State    | Free Memory | Action                                             |
| -------- | ----------- | -------------------------------------------------- |
| **High** | > 6 %       | No reclamation                                     |
| **Soft** | 4â€“6 %       | Ballooning (first) â†’ paging if needed              |
| **Hard** | 2â€“4 %       | Force paging to reclaim space                      |
| **Low**  | < 1 %       | Page heavily + temporarily block VMs above targets |

* Target allocations always computed to restore â€œHighâ€ free-space level.
* Transitions show **hysteresis** â€” system must significantly exceed upper threshold before relaxing to a less aggressive state.

---

## ğŸ§ª Experimental Demonstration â€” Dynamic Reallocation

**Setup:** 5 Windows VMs running mixed server workloads on an IBM Netfinity 8500R (8Ã— 550 MHz CPUs).

* Exchange Server + Client pair (256 MB each).
* Citrix MetaFrame Server + Client pair (320 MB each).
* SQL Server (320 MB).
* Total VM config = 1.47 GB + 160 MB overhead â‡’ > 60 % overcommit.
* min size = Â½ max; shares âˆ max.

### Observations

#### (a) Allocation States

Most time spent in **High** and **Soft** states â†’ system balances load without severe paging.

#### (b) Aggregate Behavior

* Boot phase = high memory pressure â†’ temporary paging before balloon drivers start.
* â€œ**Share before swap**â€ saves 325 MB of zero pages (shared COW), only 35 MB paged to disk.
* Total allocation â‰ˆ 1200 MB > 1 GB physical capacity due to sharing.
* During steady load â†’ page sharing saves â‰ˆ 200 MB; ballooning handles rest.

#### (c) Citrix MetaFrame Server

Allocation tracks active use closely, grows gradually as page sharing frees capacity.

#### (d) SQL Server VM

Starts high during query execution â†’ drops to 160 MB (min limit) when idle â†’ rises rapidly again when new queries begin.
Demonstrates fast reactivity and accurate working-set tracking.

---

## ğŸ§­ Summary

| Mechanism                          | Purpose                               | Outcome                            |
| ---------------------------------- | ------------------------------------- | ---------------------------------- |
| **min/max/shares**                 | Define bounds & relative rights       | Guarantees & proportional fairness |
| **Admission Control**              | Prevent overcommit beyond safe limits | Ensures reliable VM startup        |
| **Dynamic Thresholds**             | Trigger balloon/paging actions        | Adaptive pressure response         |
| **Share before Swap**              | Use page sharing before disk I/O      | Reduced paging overhead            |
| **Balloon + Page Sharing Synergy** | Complementary reclamation             | Sustained high utilization         |

**Result:** ESX Server maintains stable performance and high density even under >60 % memory overcommit, through coordinated allocation, ballooning, and page-sharing mechanisms.

---

