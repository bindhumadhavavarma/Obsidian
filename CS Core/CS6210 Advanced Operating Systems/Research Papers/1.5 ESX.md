
> makenotes

# ESX Server (Waldspurger OSDI’02) — Memory Resource Management (Notes)

## 🧭 Context & Goals

* **ESX Server**: thin VMM that **multiplexes hardware** directly (bare-metal), runs **unmodified OSes** (e.g., Windows 2000 AS, Red Hat 7.2).
* Goal: **efficient overcommit** of memory while preserving **isolation & guarantees** (statistical multiplexing without DoS).
* Differs from **VMware Workstation** (hosted): ESX bypasses host OS for I/O → **higher I/O performance** + full **resource control**.

## 🧪 Core Contributions (Memory)

1. **Ballooning**

   * Reclaims memory **from inside the guest** by inflating a driver that allocates guest pages → guest hands back **least-valuable** pages (guest-informed reclamation).

2. **Idle Memory Tax**

   * Policy that **penalizes idle pages** to achieve high utilization while maintaining **performance isolation** (discourages hoarding).

3. **Content-Based Page Sharing (CBPS)**

   * Detects **identical page contents across VMs**, remaps to a **single machine page** (copy-on-write on write) → reduces footprint **transparently**.

4. **Hot I/O Page Remapping**

   * Remaps frequently copied I/O pages to **avoid extra copies** on large-memory systems (reduces I/O copy overhead).

➡️ **Combined effect**: safely support **memory overcommit** with low overhead.

---

## 🧱 Memory Virtualization Model (Section 2)

### Terminology

* **Machine address (MPN)**: real hardware page frame.
* **“Physical” address (PPN)**: **virtualized** guest-physical frame the VM believes it owns (quoted “physical” in paper to emphasize abstraction).

### Two-Level Translation

* ESX maintains per-VM **pmap**: **PPN → MPN** mapping.
* Hardware uses **shadow page tables** containing **virtual → machine** translations (V→M), synchronized with pmap (PPN→MPN).
* Guest OS attempts to modify its own page tables / TLB are **intercepted**; ESX updates **shadow** accordingly.

### Why shadowing?

* Allows **unmodified OSes**: hardware sees valid page tables (V→M); guest still believes it manages PPNs.
* **TLB** caches V→M directly from shadow tables → **ordinary memory refs incur no extra cost** post-translation (fast steady-state).

### Power of Extra Indirection

* ESX can **transparently remap** a guest “physical” page by changing **PPN→MPN** in pmap.
* Enables:

  * **Balloon reclamation** (reassign MPNs elsewhere),
  * **Page sharing** (multiple PPNs → same MPN, with CoW),
  * **Monitoring/interposition** on guest memory accesses,
  * **Hot I/O remap** optimizations.

---

## 🔧 Mechanisms vs. Policies (paper roadmap)

* **Low-level mechanisms**: memory virtualization (shadowing), ballooning, sharing, hot I/O remap.
* **Policies**: proportional-share with **working set estimates**, **idle memory tax**, high-level **target allocations** per VM.
* Sections:

  * §2 Memory virtualization (pmap, shadows, traps),
  * §3 Reclamation (ballooning, others),
  * §4 Sharing (CBPS),
  * §5 Working-set in proportional-share,
  * §6 High-level allocation policy,
  * §7 Hot I/O page remapping,
  * §8 Related work, §9 Conclusions.

---

## 📝 Quick Compare: ESX vs. Xen (for intuition)

* **ESX (full virt)**: **shadow page tables** (V→M) + pmap (PPN→MPN); runs **unmodified** guests.
* **Xen (paravirt)**: guests manage **HW-visible page tables** but **read-only**; updates via **hypercalls** (no shadowing), needs **guest mods**.
* Trade-off: **compatibility (ESX)** vs **lower MMU overhead (Xen paravirt)**.

---

## 🔑 Takeaways

* ESX’s memory layer hinges on **an extra PPN→MPN indirection** + **shadow PTEs** to keep CPU fast while enabling rich policies.
* **Ballooning + idle tax + sharing + hot remap** = robust toolkit to **overcommit safely** with isolation and good performance for unmodified OSes.
---

> makenotes

# VMware ESX Server — Section 3: Reclamation Mechanisms

## 🧩 Overview: Memory Overcommitment

* **Overcommitment**: total configured VM memory > available physical machine memory.
* Enables **higher server consolidation** than static partitioning.
* Each VM sees a **fixed “physical” size** (max allocation) post-boot → guest OSes assume static memory.
* ESX automatically **allocates or reclaims** memory per VM based on configuration + load.

---

## ⚠️ 3.1 Page Replacement Issues

### Traditional Approach

* Older VMMs (e.g., Disco, VM/370) added a **second-level paging**:
  VM “physical” pages are **swapped to disk** by the VMM.
* Requires **meta-level replacement policy** → decides:

  1. Which VM to reclaim from.
  2. Which guest page within that VM to swap.

### Problems

1. **Limited insight**: Only the guest OS knows which pages are least valuable.

   * Each OS has its own undocumented, version-specific policies.
2. **Performance anomalies**: Meta-policy may conflict with guest paging decisions.
3. **Double paging**: both levels may evict the same page, causing:

   * VM swap-out → guest swap-out → redundant disk I/O (page pulled in, immediately swapped again).

---

## 🎈 3.2 Ballooning

### Concept

* **Balloon driver** inside guest OS acts as a mediator for memory reclamation.
* ESX inflates/deflates this balloon to adjust guest’s memory pressure.

### Mechanism

1. **Inflate** → balloon allocates pinned pages using normal OS APIs.
   → Guest OS invokes its **own replacement policy**, deciding what to evict.
2. **Deflate** → balloon frees previously allocated pages.
3. **ESX reclaims corresponding machine pages** once the balloon holds them.

> ✅ **Effect**: VM behaves as if configured with less memory — predictable, guest-aware reclamation.

### Implementation Details

* Guest balloon module = pseudo-device driver with **no user interface**.
* Communicates with ESX via **private control channel**.
* Polls ESX once per second for target size; adapts allocation rate to avoid stress.
* Typical APIs used:

  * **Linux** → `get_free_page()`
  * **Windows** → `MmAllocatePagesForMdl()`, `MmProbeAndLockPages()`
* **Fault handling**:

  * ESX annotates ballooned PPNs → deallocates MPNs.
  * If guest touches reclaimed PPN → trap handled by ESX (“pops” balloon, allocates new MPN).
  * Newly allocated pages are **zeroed** (prevent info leak) + **cache-color aware**.

### Performance

* Experiment: `dbench` (40 clients, dual-CPU Dell Precision 420, Red Hat 7.2).
* Throughput nearly identical to non-ballooned runs:

  * Overhead: **4.4% at 128 MB**, **1.4% at 224 MB**.
* Slight loss due to kernel data structures scaled by total “physical” memory (more overhead in larger VM).

### Limitations

* Balloon driver may be:

  * Unavailable (boot phase), disabled, uninstalled.
  * Too slow to reclaim under heavy pressure.
  * Bound by guest OS constraints (max alloc sizes, pinned memory rules).
* Future possibility: **hot-pluggable virtual memory cards** → coarse-grained ballooning.

---

## 🧠 3.3 Demand Paging

### When Used

* **Fallback** when ballooning is insufficient or unavailable.

### Mechanism

* ESX swap daemon:

  * Receives per-VM **target swap levels** from policy layer.
  * Selects candidate pages + performs **asynchronous page-outs**.
  * Uses **clustering** and **free-slot caching** for I/O efficiency.
* Pages swapped from **machine memory to ESX swap file** (no guest involvement).

### Policy

* **Randomized page replacement**:

  * Avoids destructive interference with guest paging algorithms.
  * Acceptable since paging is rare (last resort).
* Future work: **adaptive or per-VM policies** for more refined control.

---

## 🧾 Summary Table

| Mechanism                  | Who Decides Evictions       | Guest Involvement | Advantages                                   | Drawbacks                                            |
| -------------------------- | --------------------------- | ----------------- | -------------------------------------------- | ---------------------------------------------------- |
| **Ballooning**             | Guest OS (native algorithm) | Yes               | Predictable perf, guest-aware                | Requires balloon driver                              |
| **Demand Paging**          | ESX meta-policy             | No                | Works even if guest unmodified/uncooperative | Limited insight, potential double paging if frequent |
| **Randomized Replacement** | ESX (fallback)              | No                | Avoids interference                          | Less optimal reuse decisions                         |

---

### 🧩 Key Takeaway

> ESX Server reclaims memory **intelligently** using **ballooning** as the cooperative first line and **paging** as the safety net — combining guest-aware decisions with hypervisor-level control for efficient and safe overcommitment.

---
> makenotes

# VMware ESX Server — Section 4: Sharing Memory

## 🧠 Overview

Server consolidation naturally creates **memory redundancy** across VMs — e.g., same OS kernels, applications, or data pages loaded in multiple instances.
**ESX Server** exploits this by transparently identifying identical pages and allowing multiple VMs to share the same machine page.
This **reduces total memory footprint**, allowing **higher overcommitment** while maintaining performance.

---

## 🔍 4.1 Transparent Page Sharing (Background – from Disco)

* First introduced by **Disco** (Stanford VMM project) to eliminate redundant copies across VMs.
* **Mechanism**: identical pages → mapped to a **single machine page**, marked **copy-on-write (COW)**.

  * Write → page fault → private copy created.
* **Limitation (Disco)**:

  * Required **guest OS modifications** to identify and hook into page-creation routines (e.g., `bcopy()` for buffer sharing).
  * Relied on **non-standard interfaces** (e.g., large-packet virtual network interfaces, shared nonpersistent disks).
  * Not feasible in ESX’s environment where **unmodified OSes must run**.

---

## 🧩 4.2 Content-Based Page Sharing (ESX’s Approach)

### Core Idea

* Identify identical pages **purely by their contents**, not by how/where they were created.
* Uses **content hashing** rather than OS-level hooks.
* Advantages:

  * No need to modify or understand guest OS or APIs.
  * Works across all workloads, regardless of application behavior.

### Algorithm

1. **Scan candidate pages** and compute a **64-bit hash** for each.
2. Use the hash as a lookup key in a **global hash table** of already shared (COW) pages.
3. If a hash match is found → perform **byte-by-byte comparison** to confirm.

   * If identical → update guest’s mapping: multiple PPNs → one shared MPN (marked COW).
   * If not identical → page remains unshared (stored as a *hint* for future matches).

> ✅ **Optimization:**
> Pages not matched are not immediately marked COW — instead stored as *hints*.
>
> * On rehash, if content changed → hint discarded.
> * If unchanged → full compare, then shared.

### Scanning Policy

* **Random or incremental scanning** of pages (configurable rate).
* Can prioritize:

  * Pages marked **read-only**.
  * Pages containing **executable code**.
* To limit overhead, scans often occur **during idle CPU cycles**.

---

## ⚙️ 4.3 Implementation Details

### Data Structures

* **Global hash table** (handles all scanned pages):

  * Entries (frames) = 16 bytes each.
  * Two frame types:

    * **Shared frame:** stores full hash, MPN, ref count, and chain link.
    * **Hint frame:** stores truncated hash + back-reference (VM ID + PPN).
* **Reference counting:**

  * Each shared page tracks number of VMs referencing it.
  * Overflow handled in a secondary table (for heavily shared pages, e.g., all-zero pages).
* **Hashing:**

  * Fast, high-quality 64-bit hash function used.
  * Probability of collision in 64 GB memory < 0.01%.
* **Overhead:**

  * Total memory overhead < **0.5% of system memory**.

### Optimizations

* Share-before-swap: ESX tries to **share a page before paging it out**.
* Random scanning keeps CPU overhead negligible.

---

## 📊 4.4 Experimental Results

### 🧪 Controlled “Best Case” — Homogeneous VMs

* Setup: 1–10 identical Linux VMs (40 MB each) running SPEC95 for 30 mins.
* Hardware: Dell PowerEdge 1400SC, dual 933 MHz Pentium III CPUs.

#### Results (Figure 4)

| Metric         | Observation                                                      |
| -------------- | ---------------------------------------------------------------- |
| Single VM      | 5 MB reclaimed (55% = zero pages)                                |
| Increasing VMs | Sharing grows **linearly** with VM count                         |
| With 10 VMs    | **67% of VM memory shared**, **~60% reclaimed**                  |
| Overhead       | Negligible (≤ 0.5% difference in CPU-bound benchmark throughput) |

> 🔎 Most sharing from **redundant code and read-only data**, not zero pages.

### 🏭 Real-World Deployments (Figure 5)

| Workload | VMs | OS         | Shared MB (%) | Reclaimed MB (%) | Notes                      |
| -------- | --- | ---------- | ------------- | ---------------- | -------------------------- |
| **A**    | 10  | Windows NT | 880 (42.9%)   | 673 (32.9%)      | Mixed DB/web/dev workloads |
| **B**    | 9   | Linux      | 539 (29.2%)   | 345 (18.7%)      | Mail + web servers         |
| **C**    | 5   | Linux      | 165 (10%)     | 120 (7.2%)       | Web proxy, mail, SSH       |

* **Zero pages** contributed a portion (e.g., 70 MB in workload B).
* CPU overhead: negligible to slightly positive (improved cache locality).

---

## 🧾 Summary Table

| Aspect                   | Disco                   | ESX Server                            |
| ------------------------ | ----------------------- | ------------------------------------- |
| Requires OS modification | ✅ Yes                   | ❌ No                                  |
| Identification method    | Hooked routines         | Content-based hashing                 |
| Type of pages shared     | OS + buffer cache pages | Any identical content                 |
| Copy-on-write            | Yes                     | Yes                                   |
| Overhead                 | Moderate                | < 0.5%                                |
| Typical savings          | N/A                     | 7–67% of memory depending on workload |

---

## 🧩 Key Takeaways

* **Content-based page sharing** is a cornerstone of ESX’s efficiency: it enables **automatic deduplication** of identical pages across unmodified VMs.
* Works seamlessly without requiring any cooperation from the guest OS or applications.
* Real-world results confirm **substantial memory savings** with minimal CPU cost, making high levels of **safe overcommitment** practical in production environments.
---

> makenotes

# VMware ESX Server – Section 5: Shares vs Working Sets

## 🎯 Goal

Balance **memory efficiency** with **performance isolation** across virtual machines (VMs).
Traditional OSes optimize global throughput, but ESX Server introduces **share-based memory management** and an **idle-memory tax** to enforce fairness while improving utilization.

---

## ⚖️ 5.1 Share-Based Allocation

* **Shares** represent proportional rights to memory.

  * Each VM receives memory proportional to its share count.
  * If total demand exceeds supply, allocations scale down proportionally.
  * Over-provisioned VMs release space to those with higher *shares per page* (a “price” metric).
* **Min-funding revocation algorithm:**

  * Chooses a “victim” VM with the fewest shares per allocated page.
  * Memory is “sold” from low-priority (low-price) VMs to higher-priority ones.
* Graceful degradation → resource rights degrade smoothly under load, guaranteeing minimum fractions while allowing dynamic growth.

---

## 💤 5.2 Reclaiming Idle Memory

**Problem:** Pure proportional allocation wastes space—idle high-share VMs hoard memory while active low-share VMs starve.

**Solution:** ESX introduces an **Idle Memory Tax (IMT)**

* **Idea:** Charge more for pages that aren’t actively used.

* When memory is scarce → reclaim from idle VMs first.

* **Tax Rate (τ):**

  * Controls aggressiveness of reclamation.
  * τ = 0 → pure share-based (no reclaim).
  * τ = 1 → reclaim all idle pages.

* Default τ = 75 % — balances responsiveness with buffer for sudden workload growth.

* Adjusted shares-per-page ratio α:

  [
  α = \frac{shares}{(active pages + (1 − τ) × idle pages)}
  ]

* Effect → idle memory gradually reclaimed; active VMs retain priority; policy easily tunable.

---

## 📊 5.3 Measuring Idle Memory (Working Set Estimation)

**Goal:** Estimate fraction of each VM’s memory that’s actively used — without guest OS cooperation.

### Implementation

* **Sampling** approach (statistical, not intrusive):

  1. Periodically select ≈ 100 guest pages every 30 s of VM runtime.
  2. Invalidate their mappings → next access triggers a minor fault.
  3. Count “touched” pages = active subset estimate (≈ touched / sampled).
* **Advantages:**

  * Works across any OS (no special APIs).
  * Ignores DMA issues that bypass page-table bits.
  * Overhead negligible (< 100 minor faults / 30 s).

### Smoothing and Responsiveness

* Maintains **three moving averages** of activity:

  * **Fast** → tracks rapid spikes.
  * **Slow** → provides stability.
  * **Current-period fast** → captures in-progress surges.
* Final estimate = **max(fast, slow, current)**
  → fast response to increased activity; gradual decay for inactivity.

---

## 🧪 5.4 Experimental Results

### Memory Sampling (Windows VM)

* Windows 2000 VM with a “memory toucher” app shows ESX accurately tracking working set size.
* Fast average tracks growth; slow average lags during decrease → desired hysteresis.
* Unexpected spike after workload ends = Windows **zero page thread** zeroing freed pages.

### Idle Memory Tax Effectiveness

* Setup: Two VMs (each 256 MB, equal shares) on 512 MB server.

  * VM1 = idle Windows 2000.
  * VM2 = active Linux (dbench workload).
* At τ = 0 % (pure shares): both get ≈ 179 MB allocation → inefficient.
* Raise τ → 75 % → idle VM loses memory, active VM gains memory → throughput ↑ by 30 %.

---

## 🧭 Summary

| Mechanism              | Purpose                    | Key Idea                                 | Outcome                  |
| ---------------------- | -------------------------- | ---------------------------------------- | ------------------------ |
| **Shares**             | Guarantee fairness         | Allocate memory proportional to weights  | Predictable isolation    |
| **Idle Memory Tax**    | Boost utilization          | Penalize idle pages, reclaim them first  | Active VMs get priority  |
| **Sampling**           | Estimate usage             | Randomly invalidate pages → track access | OS-independent accuracy  |
| **Adaptive Averaging** | Stability + Responsiveness | Combine fast & slow EWMA                 | Smooth allocation shifts |

**Result:** ESX Server achieves efficient, fair, and adaptive memory distribution across VMs — maintaining QoS isolation while maximizing hardware utilization.

---

> makenotes

# VMware ESX Server – Section 6: Allocation Policies

## ⚙️ Overview

ESX Server dynamically manages VM memory through a coordinated system of **share-based entitlements**, **working-set estimates**, and **reclamation mechanisms** (ballooning + paging + page-sharing).
The goal is to keep host memory utilization high while upholding isolation and fairness guarantees.

---

## 🧩 6.1 Parameters — Administrator Controls

Each VM is configured with three key parameters:

| Parameter    | Meaning                      | Guarantee                             |
| ------------ | ---------------------------- | ------------------------------------- |
| **min size** | Guaranteed lower bound       | Always allocated, even under pressure |
| **max size** | Configured “physical” memory | Upper limit visible to guest OS       |
| **shares**   | Relative entitlement weight  | Determines proportional allocation    |

* When memory is plentiful → each VM gets its **max size**.
* When overcommitted → allocations shrink toward **min**, proportional to shares and activity.
* Two VMs with 2:1 share ratio get approximately 2× and 1× memory respectively, subject to their bounds.

---

## 🛡️ 6.2 Admission Control

Before power-on, ESX verifies sufficient resources:

* **Machine memory reservation = min + overhead** (per-VM metadata + graphics frame buffer + pmap/shadow tables).

  * Typical overhead ≈ 32 MB (4–8 MB for frame buffer).
* **Swap space reservation = max − min** → guarantees VM state preservation if all memory must be paged.
* Only a small fraction is usually used at runtime.
* Reservations control admission but do not statically tie up memory — unused space can be reallocated.

---

## 🔄 6.3 Dynamic Reallocation

ESX continuously adjusts allocations based on:

* Parameter changes (admin edits, VM start/stop).
* Free-memory threshold crossings.
* Periodic updates from idle-memory estimates.

### Reclamation Thresholds (Defaults in % of system RAM)

| State    | Free Memory | Action                                             |
| -------- | ----------- | -------------------------------------------------- |
| **High** | > 6 %       | No reclamation                                     |
| **Soft** | 4–6 %       | Ballooning (first) → paging if needed              |
| **Hard** | 2–4 %       | Force paging to reclaim space                      |
| **Low**  | < 1 %       | Page heavily + temporarily block VMs above targets |

* Target allocations always computed to restore “High” free-space level.
* Transitions show **hysteresis** — system must significantly exceed upper threshold before relaxing to a less aggressive state.

---

## 🧪 Experimental Demonstration — Dynamic Reallocation

**Setup:** 5 Windows VMs running mixed server workloads on an IBM Netfinity 8500R (8× 550 MHz CPUs).

* Exchange Server + Client pair (256 MB each).
* Citrix MetaFrame Server + Client pair (320 MB each).
* SQL Server (320 MB).
* Total VM config = 1.47 GB + 160 MB overhead ⇒ > 60 % overcommit.
* min size = ½ max; shares ∝ max.

### Observations

#### (a) Allocation States

Most time spent in **High** and **Soft** states → system balances load without severe paging.

#### (b) Aggregate Behavior

* Boot phase = high memory pressure → temporary paging before balloon drivers start.
* “**Share before swap**” saves 325 MB of zero pages (shared COW), only 35 MB paged to disk.
* Total allocation ≈ 1200 MB > 1 GB physical capacity due to sharing.
* During steady load → page sharing saves ≈ 200 MB; ballooning handles rest.

#### (c) Citrix MetaFrame Server

Allocation tracks active use closely, grows gradually as page sharing frees capacity.

#### (d) SQL Server VM

Starts high during query execution → drops to 160 MB (min limit) when idle → rises rapidly again when new queries begin.
Demonstrates fast reactivity and accurate working-set tracking.

---

## 🧭 Summary

| Mechanism                          | Purpose                               | Outcome                            |
| ---------------------------------- | ------------------------------------- | ---------------------------------- |
| **min/max/shares**                 | Define bounds & relative rights       | Guarantees & proportional fairness |
| **Admission Control**              | Prevent overcommit beyond safe limits | Ensures reliable VM startup        |
| **Dynamic Thresholds**             | Trigger balloon/paging actions        | Adaptive pressure response         |
| **Share before Swap**              | Use page sharing before disk I/O      | Reduced paging overhead            |
| **Balloon + Page Sharing Synergy** | Complementary reclamation             | Sustained high utilization         |

**Result:** ESX Server maintains stable performance and high density even under >60 % memory overcommit, through coordinated allocation, ballooning, and page-sharing mechanisms.

---

