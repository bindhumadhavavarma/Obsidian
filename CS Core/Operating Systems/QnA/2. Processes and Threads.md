
**1. In Fig. 2-2, three process states are shown. In theory, with three states, there could be six transitions, two out of each state. However, only four transitions are shown. Are there any circumstances in which either or both of the missing transitions might occur?**  
Yes, the two missing transitions are:  
- **Blocked → Ready**: When an I/O operation completes.  
- **Running → Ready**: When the running process is preempted by the scheduler.

---

**2. Suppose that you were to design an advanced computer architecture that did process switching in hardware, instead of having interrupts. What information would the CPU need? Describe how the hardware process switching might work.**  
The CPU would need process control blocks (PCBs) containing registers, program counters, and stack pointers. Hardware would detect events like I/O completion, save the current process state, restore another process state from PCB, and switch contexts without OS software intervention.

---

**3. On all current computers, at least part of the interrupt handlers are written in assembly language. Why?**  
Interrupt handlers require direct hardware manipulation, precise timing, and register-level control, which high-level languages cannot provide. Assembly gives the low-level control needed during early interrupt processing.

---

**4. When an interrupt or a system call transfers control to the operating system, a kernel stack area separate from the stack of the interrupted process is generally used. Why?**  
To prevent user stack corruption, ensure kernel safety, and allow OS operations even if the user stack is full or invalid.

---

**5. A computer system has enough room to hold four programs in its main memory. These programs are idle waiting for I/O half the time. What fraction of the CPU time is wasted?**  
Probability all four are waiting:  
\[(0.5)^4 = 0.0625 = 6.25%\]  
So, **6.25% of CPU time is wasted**, and **93.75% is utilized**.

---

**6. A computer has 2 GB of RAM of which the operating system occupies 256 MB. The processes are all 128 MB (for simplicity) and have the same characteristics. If the goal is 99% CPU utilization, what is the maximum I/O wait that can be tolerated?**  
Available RAM: 2 GB - 256 MB = 1792 MB  
Processes in memory: ⌊1792 / 128⌋ = 14  
Let p = I/O wait fraction. Then:  
\[p^{14} ≤ 0.01\]  
\[\log(p) ≤ \frac{-2}{14} = -0.1429\]  
\[p ≤ 10^{-0.1429} ≈ 0.72\]  
**Maximum I/O wait = 72%**

---

**7. Multiple jobs can run in parallel and finish faster than if they had run sequentially. Suppose that two jobs, each needing 10 minutes of CPU time, start simultaneously. How long will the last one take to complete if they run sequentially? How long if they run in parallel? Assume 50% I/O wait.**  
- **Sequential:** Each job takes 20 minutes (10 CPU + 10 I/O). Total = 40 minutes.  
- **Parallel:** While one is doing I/O, the other uses CPU. Effective CPU usage = 50%. So both finish in 20 minutes.  
**Answer:** Sequential = 40 min, Parallel = 20 min.

---

**8. Consider a multiprogrammed system with degree of 5 (i.e., five programs in memory at the same time). Assume that each process spends 40% of its time waiting for I/O. What will be the CPU utilization?**  
Let p = 0.4 (I/O wait), n = 5.  
CPU utilization = 1 - pⁿ = 1 - (0.4)^5 = 1 - 0.01024 = **0.98976 or 98.976%**

---

**9. Explain how a Web browser can utilize the concept of threads to improve performance.**  
Each browser tab or task (e.g., rendering, fetching data) can be handled by separate threads:  
- Allows parallel page loading  
- UI remains responsive while content loads  
- Independent processing of JavaScript, images, etc.

---

**10. Assume that you are trying to download a large 2-GB file from the Internet. The file is available from a set of mirror servers, each of which can deliver a subset of the file’s bytes; assume that a given request specifies the starting and ending bytes of the file. Explain how you might use threads to improve the download time.**  
Create multiple threads, each downloading a different byte range from different mirrors in parallel. This:  
- Utilizes available bandwidth  
- Overlaps latency  
- Downloads file segments concurrently  
- Finally, merge the chunks into a single file.

---

**11. In the text it was stated that the model of Fig. 2-10(a) was not suited to a file server using a cache in memory. Why not? Could each process have its own cache?**  
Fig. 2-10(a) uses separate processes with no shared memory.  
- A shared cache is needed for efficiency; separate process caches would lead to inconsistency and redundancy.  
- So no, each process having its own cache would not work well.

---

**12. In Fig. 2-8, a multithreaded Web server is shown. If the only way to read from a file is the normal blocking read system call, do you think user-level threads or kernel-level threads are being used for the Web server? Why?**  
Kernel-level threads are likely being used, because:  
- Blocking read in one thread would block the entire process if using user-level threads.  
- Kernel threads allow blocking in one thread while others continue execution.

---

**13. In the text, we described a multithreaded Web server, showing why it is better than a single-threaded server and a finite-state machine server. Are there any circumstances in which a single-threaded server might be better? Give an example.**  
Yes. In low-traffic environments or embedded systems:  
- Less overhead  
- Simpler design  
- No need for context switching or synchronization

---

**14. In Fig. 2-11 the register set is listed as a per-thread rather than a per-process item. Why? After all, the machine has only one set of registers.**  
Each thread needs its own virtual register set saved during context switches.  
- Although the physical CPU has one set, each thread has its own copy in the thread control block (TCB).  
- Needed to resume threads correctly.

---

**15. Why would a thread ever voluntarily give up the CPU by calling thread yield? After all, since there is no periodic clock interrupt, it may never get the CPU back.**  
Used in cooperative multitasking to:  
- Improve responsiveness  
- Avoid starvation  
- Allow fair use of CPU when threads trust each other to yield

---

**16. In this problem, you are to compare reading a file using a single-threaded file server and a multithreaded server. It takes 15 msec to get a request for work, dispatch it, and do the rest of the necessary processing, assuming that the data needed are in the block cache. If a disk operation is needed, as is the case one-third of the time, an additional 75 msec is required, during which time the thread sleeps. How many requests/sec can the server handle if it is single threaded? If it is multithreaded?**  
- **Single-threaded:**  
  Avg time/request = (2/3)(15) + (1/3)(90) = 10 + 30 = 40 msec  
  → 1000 / 40 = **25 requests/sec**

- **Multithreaded:**  
  During 75 msec disk I/O, another thread can handle next request.  
  → Only 15 msec of active CPU per request  
  → 1000 / 15 = **~66.67 requests/sec**

---

**17. What is the biggest advantage of implementing threads in user space? What is the biggest disadvantage?**  
- **Advantage:** Fast thread creation, termination, and switching (no kernel involvement).  
- **Disadvantage:** Blocking system calls block the entire process since the kernel is unaware of multiple threads.

---

**18. In Fig. 2-14 the thread creations and messages printed by the threads are interleaved at random. Is there a way to force the order to be strictly thread 1 created, thread 1 prints message, thread 1 exits, thread 2 created, thread 2 prints message, thread 2 exists, and so on? If so, how? If not, why not?**  
Yes, by using synchronization primitives (e.g., semaphores or condition variables) to enforce strict ordering between thread creation and execution.

---

**19. Suppose that a program has two threads, each executing the get account function, shown below. Identify a race condition in this code.**  
```c
int accounts[LIMIT]; 
int account_count = 0;

void *get_account(void *tid) {
    char *lineptr = NULL;
    size_t len = 0;
    while (account_count < LIMIT) {
        getline(&lineptr, &len, stdin);
        int entered_account = atoi(lineptr);
        accounts[account_count] = entered_account;
        account_count++;
    }
    free(lineptr);
    return NULL;
}
```  
Race condition: `account_count` is accessed and updated by both threads without synchronization. This can cause incorrect indexing or overwriting data. Use a mutex to protect access to `account_count`.

---

**20. In the discussion on global variables in threads, we used a procedure create_global to allocate storage for a pointer to the variable, rather than the variable itself. Is this essential, or could the procedures work with the values themselves just as well?**  
It's essential for sharing and modifying variables across threads. Allocating a pointer ensures a shared memory location, whereas copying values would result in each thread having a separate copy, breaking shared semantics.

---

**21. Consider a system in which threads are implemented entirely in user space, with the run-time system getting a clock interrupt once a second. Suppose that a clock interrupt occurs exactly while some thread executing in the run-time system is at the point of blocking or unblocking a thread. What problem might occur? Can you solve it?**  
The interrupt may cause the scheduler to switch threads while the data structures for thread blocking/unblocking are in an inconsistent state. This could lead to data corruption or undefined behavior.  
**Solution:** Disable interrupts (or block signals) while modifying thread management structures to ensure atomicity.

---

**22. Suppose that an operating system does not have anything like the select system call to see in advance if it is safe to read from a file, pipe, or device, but it does allow alarm clocks (timers) to be set that interrupt blocked system calls. Is it possible to implement in user space a threads package that will not block all threads when one thread performs a system call that may block? Explain your answer.**  
Yes, use a timer to interrupt a potentially blocking system call after a short duration. If the call blocks, the alarm causes it to return early, allowing the scheduler to switch threads. However, this is inefficient and unreliable due to the complexity of handling partial calls and resuming safely.

---

**23. Does Peterson’s solution to the mutual-exclusion problem shown in Fig. 2-24 work when process scheduling is preemptive? How about when it is nonpreemptive?**  
Yes, Peterson’s solution works in both preemptive and nonpreemptive environments, as long as memory accesses to shared variables are atomic. It does not depend on the scheduling policy.

---

**24. Can the priority inversion problem discussed in Sec. 2.3.4 happen with user-level threads? Why or why not?**  
Yes, if the thread scheduler respects priorities. A low-priority thread holding a resource needed by a high-priority thread may block it indefinitely, unless priority inheritance is implemented in the user-level scheduler.

---

**25. In Sec. 2.3.4, a situation with a high-priority process, H, and a low-priority process, L, was described, which led to H looping forever. Does the same problem occur if round-robin scheduling is used instead of priority scheduling? Discuss.**  
No, round-robin scheduling gives each process a fair time slice, so L will eventually release the resource, allowing H to proceed. Priority inversion occurs in strict priority-based schedulers without proper handling.

---

**26. In a system with threads, is there one stack per thread or one stack per process when user-level threads are used? What about when kernel-level threads are used? Explain.**  
- **User-level threads:** Each thread has its own stack in the process's address space.  
- **Kernel-level threads:** Each thread has its own stack maintained by the OS.  
This isolation allows function calls and local variables to be managed independently.

---

**27. What is a race condition?**  
A race condition occurs when multiple threads or processes access shared data concurrently, and the outcome depends on the timing of their execution. Without proper synchronization, it can lead to unpredictable behavior or errors.

---

**28. When a computer is being developed, it is usually first simulated by a program that runs one instruction at a time. Even multiprocessors are simulated strictly sequentially like this. Is it possible for a race condition to occur when there are no simultaneous events? Explain.**  
Yes, race conditions can still occur if shared variables are accessed without synchronization. The interleaving of operations (even in a sequential simulation) can lead to inconsistent states if operations are not atomic.

---

**29. The producer-consumer problem can be extended to a system with multiple producers and consumers that write (or read) to (from) one shared buffer. Assume that each producer and consumer runs in its own thread. Will the solution presented in Fig. 2-28, using semaphores, work for this system?**  
Yes, the semaphore-based solution is generalizable to multiple producers and consumers, as long as mutual exclusion is properly enforced on the buffer with a mutex, and empty/full semaphores are used correctly.

---

**30. Consider the following solution to the mutual-exclusion problem involving two processes P0 and P1. Assume that the variable turn is initialized to 0.**  
```c
/* Other code */
while (turn != 0) { } /* Do nothing and wait. */
Critical Section
turn = 0;
/* Other code */
```
For process P1, replace 0 by 1 in above code. Determine if the solution meets all the required conditions for a correct mutual-exclusion solution.  
No, this solution does **not** satisfy mutual exclusion. Both processes can enter the critical section if they check `turn` simultaneously before either sets it. It lacks proper intention signaling and progress control.

---

**31. Show how counting semaphores (i.e., semaphores that can hold an arbitrary value) can be implemented using only binary semaphores and ordinary machine instructions.**  
Use:
- A binary semaphore `mutex` to protect access to a counter variable.
- A queue or additional binary semaphores to manage blocked processes.
Implement `wait()` by decrementing the counter under mutex and blocking if it's < 0.  
Implement `signal()` by incrementing and waking a blocked process if any.

---

**32. If a system has only two processes, does it make sense to use a barrier to synchronize them? Why or why not?**  
Yes. A barrier ensures both processes reach a certain point before proceeding. This can be useful even with just two processes, e.g., for phased computation or synchronization checkpoints.

---

**33. Can two threads in the same process synchronize using a kernel semaphore if the threads are implemented by the kernel? What if they are implemented in user space? Assume that no threads in any other processes have access to the semaphore. Discuss your answers.**  
- **Kernel threads:** Yes, kernel semaphores can synchronize them.  
- **User-level threads:** No, kernel semaphores won’t help, as the kernel sees only one process. Use user-space semaphores instead.

---

**34. Suppose that we have a message-passing system using mailboxes. When sending to a full mailbox or trying to receive from an empty one, a process does not block. Instead, it gets an error code back. The process responds to the error code by just trying again, over and over, until it succeeds. Does this scheme lead to race conditions?**  
Yes, this busy-waiting approach can lead to race conditions, especially if multiple processes check the mailbox and then act based on outdated state before the actual operation.

---

**35. The CDC 6600 computers could handle up to 10 I/O processes simultaneously using an interesting form of round-robin scheduling called processor sharing. A process switch occurred after each instruction, so instruction 1 came from process 1, instruction 2 came from process 2, etc. The process switching was done by special hardware, and the overhead was zero. If a process needed T sec to complete in the absence of competition, how much time would it need if processor sharing was used with n processes?**  
Each process gets 1/n of the CPU. So, it will need **T × n** seconds to complete.

---

**36. Consider the following piece of C code:**
```c
void main() {
    fork();
    fork();
    exit();
}
```
**How many child processes are created upon execution of this program?**  
Two `fork()` calls result in **2^2 = 4 processes** total. Since one is the original, the number of **child** processes is **3**.

---

**37. Round-robin schedulers normally maintain a list of all runnable processes, with each process occurring exactly once in the list. What would happen (scheduling-wise) if a process occurred twice in the list? Can you think of any reason for allowing this?**  
It would get more CPU time, effectively giving it higher priority. This technique can be used to simulate weighted round-robin or priority scheduling.

---

**38. Can a measure of whether a process is likely to be CPU bound or I/O bound be determined by analyzing source code? How can this be determined at run time?**  
- **Compile-time:** Hard to determine reliably via static analysis.  
- **Run-time:** Monitor CPU vs. I/O wait time ratios. CPU-bound processes use more CPU time between I/O operations.

---

**39. In the section ‘‘When to Schedule,’’ it was mentioned that sometimes scheduling could be improved if an important process could play a role in selecting the next process to run when it blocks. Give a situation where this could be used and explain how.**  
In client-server systems, a server process can prioritize serving important clients by selecting the next thread based on client importance when it blocks waiting for I/O or a response.

---

**40. Explain how time quantum value and context switching time affect each other, in a round-robin scheduling algorithm.**  
If the time quantum is too small compared to context switch time, more time is wasted in overhead. If it's too large, responsiveness suffers. Ideal quantum balances throughput and interactivity, minimizing context switch overhead as a fraction of total CPU time.

---

**41. Measurements of a certain system have shown that the average process runs for a time T before blocking on I/O. A process switch requires a time S, which is effectively wasted (overhead). For round-robin scheduling with quantum Q, give a formula for the CPU efficiency for each of the following:**  
Let efficiency = useful work / total time = T / (T + overhead)

(a) **Q = ∞**  
No context switches.  
Efficiency = **1**

(b) **Q > T**  
Process blocks before quantum expires.  
Efficiency = **T / (T + S)**

(c) **S < Q < T**  
Each process runs for Q, switches due to quantum expiration.  
Efficiency = **Q / (Q + S)**

(d) **Q = S**  
Efficiency = **Q / (Q + Q) = 0.5**

(e) **Q nearly 0**  
Context switching dominates.  
Efficiency ≈ **0**

---

**42. Five jobs are waiting to be run. Their expected run times are 9, 6, 3, 5, and X. In what order should they be run to minimize average response time? (Your answer will depend on X.)**  
Use **Shortest Job First (SJF)**. Sort by ascending run time:
- If **X ≤ 3**: order = X, 3, 5, 6, 9  
- If **3 < X ≤ 5**: order = 3, X, 5, 6, 9  
- If **5 < X ≤ 6**: order = 3, 5, X, 6, 9  
- If **6 < X ≤ 9**: order = 3, 5, 6, X, 9  
- If **X > 9**: order = 3, 5, 6, 9, X

---

**43. Five batch jobs, A through E, arrive at almost the same time. They have estimated running times of 10, 6, 2, 4, and 8 minutes. Their (externally determined) priorities are 3, 5, 2, 1, and 4, respectively, with 5 being the highest priority. For each of the following scheduling algorithms, determine the mean process turnaround time. Ignore process switching overhead.**

Jobs:  
A (10, priority 3)  
B (6, priority 5)  
C (2, priority 2)  
D (4, priority 1)  
E (8, priority 4)

(a) **Round robin (equal time slices, fair CPU share)**  
Each job gets CPU time in round-robin fashion. All finish at similar time.  
Turnaround ≈ (Total time) / 2 = (10+6+2+4+8)/2 = 30/2 = **15 min**

(b) **Priority scheduling (highest number = highest priority)**  
Order: B (6), E (8), A (10), C (2), D (4)  
Finish times: B-6, E-14, A-24, C-26, D-30  
Turnarounds: 6, 14, 24, 26, 30 → avg = **20**

(c) **First-come, first-served (order: A, B, C, D, E)**  
Finish times: A-10, B-16, C-18, D-22, E-30  
Avg = (10 + 16 + 18 + 22 + 30) / 5 = **19.2**

(d) **Shortest job first (order: C, D, B, E, A)**  
Finish times: C-2, D-6, B-12, E-20, A-30  
Avg = (2 + 6 + 12 + 20 + 30) / 5 = **14**

---

**44. A process running on CTSS needs 30 quanta to complete. How many times must it be swapped in, including the very first time (before it has run at all)?**  
CTSS swaps a process in and out **each quantum**.  
Swapped in once per quantum → **30 times**

---

**45. Can you think of a way to save the CTSS priority system from being fooled by random carriage returns?**  
Yes. Update user priority based on actual CPU usage or real activity (like command execution), not on number of carriage returns alone. Use time-based decay or weighted input parsing to avoid abuse.

---

**46. Consider a real-time system with two voice calls of periodicity 5 msec each with CPU time per call of 1 msec, and one video stream of periodicity 33 msec with CPU time per call of 11 msec. Is this system schedulable? Show how you derived your answer.**  
CPU utilization:  
Voice: 2 × (1/5) = 0.4  
Video: 11/33 ≈ 0.333  
Total = 0.4 + 0.333 = **0.733**

Since total utilization < 1, **yes, the system is schedulable**.

---

**47. For the above problem, can another video stream be added and have the system still be schedulable?**  
Adding another identical video stream adds 0.333 more utilization.  
New total: 0.733 + 0.333 = **1.066** > 1  
**No**, the system would no longer be schedulable.

---

**48. The aging algorithm with α = 1/2 is being used to predict run times. The previous four runs, from oldest to most recent, are 40, 20, 40, and 15 msec. What is the prediction of the next time?**  
Use exponential average:  
Tₙ₊₁ = α × Tₙ + (1 - α) × T̂ₙ  
Start with T̂₀ = 40  
→ T̂₁ = 0.5×20 + 0.5×40 = 30  
→ T̂₂ = 0.5×40 + 0.5×30 = 35  
→ T̂₃ = 0.5×15 + 0.5×35 = **25**

Prediction = **25 msec**

---

**49. A soft real-time system has four periodic events with periods of 50, 100, 200, and 250 msec each. Suppose that the four events require 35, 20, 10, and x msec of CPU time, respectively. What is the largest value of x for which the system is schedulable?**  
Utilization:  
35/50 = 0.7  
20/100 = 0.2  
10/200 = 0.05  
Let x/250 ≤ remaining CPU time.  
0.7 + 0.2 + 0.05 + x/250 ≤ 1  
x/250 ≤ 0.05 → x ≤ **12.5**

Max x = **12 msec**

---

**50. Explain why two-level scheduling is commonly used. What advantages does it have over single-level scheduling?**  
- Separates long-term (which processes enter memory) and short-term (which runs next) scheduling.  
- Balances CPU and I/O-bound jobs, improves throughput.  
- Reduces thrashing by controlling degree of multiprogramming.

---

**51. A real-time system needs to handle two voice calls that each run every 5 msec and consume 1 msec of CPU time per burst, plus one video at 25 frames/sec, with each frame requiring 20 msec of CPU time. Is this system schedulable? Please explain why or why not it is schedulable and how you came to that conclusion.**  
Voice: 2 × (1/5) = 0.4  
Video: 25 fps → 1 frame every 40 ms → 20/40 = 0.5  
Total utilization = 0.4 + 0.5 = **0.9**  
Since total < 1, **the system is schedulable**.

---

**52. Consider a system in which it is desired to separate policy and mechanism for the scheduling of kernel threads. Propose a means of achieving this goal.**  
Implement a generic scheduler in the kernel (mechanism) that supports hooks or pluggable modules. Scheduling policy (e.g., priority, round-robin) is defined in user space or as loadable modules, allowing flexibility without changing kernel mechanisms.

---

**53. The readers and writers problem can be formulated in several ways with regard to which category of processes can be started when. Carefully describe three different variations of the problem, each one favoring (or not favoring) some category of processes (e.g., readers or writers). For each variation, specify what happens when a reader or a writer becomes ready to access the database, and what happens when a process is finished.**  
1. **Reader priority:** Readers can access if no writer is writing. Writers must wait if any reader is active.  
2. **Writer priority:** Writer starts as soon as possible. No new readers can start once a writer is waiting.  
3. **Fairness (FIFO):** Readers and writers are queued in order of arrival. No starvation.

---

**54. Write a shell script that produces a file of sequential numbers by reading the last number in the file, adding 1 to it, and then appending it to the file. Run one instance of the script in the background and one in the foreground, each accessing the same file. How long does it take before a race condition manifests itself? What is the critical region? Modify the script to prevent the race. (Hint: use `ln file file.lock` to lock the data file.)**  
Race condition likely appears quickly, possibly within a few runs.  
**Critical region:** reading the last number and writing the new one.  
**Fix:**  
```sh
while true; do
  ln numbers.txt numbers.lock 2>/dev/null || continue
  last=$(tail -n 1 numbers.txt)
  echo $((last + 1)) >> numbers.txt
  rm numbers.lock
  break
done
```

---

**55. Assume that you have an operating system that provides semaphores. Implement a message system. Write the procedures for sending and receiving messages.**  
Use a shared buffer, a `mutex` for mutual exclusion, and semaphores `full` and `empty`.  
**Send:**  
```c
wait(empty);
wait(mutex);
enqueue(message);
signal(mutex);
signal(full);
```
**Receive:**  
```c
wait(full);
wait(mutex);
dequeue();
signal(mutex);
signal(empty);
```

---

**56. Rewrite the program of Fig. 2-23 to handle more than two processes.**  
Use a `flag[]` array and a `turn` variable for `n` processes.  
Extend Peterson’s algorithm into the **Filter Algorithm** for n processes, ensuring mutual exclusion using levels and turn arrays.

---

**57. Write a producer-consumer problem that uses threads and shares a common buffer. However, do not use semaphores or any other synchronization primitives to guard the shared data structures. Just let each thread access them when it wants to. Use sleep and wakeup to handle the full and empty conditions. See how long it takes for a fatal race condition to occur.**  
Race conditions appear quickly. Without mutex/semaphore, simultaneous access corrupts the buffer. Sleep/wakeup are insufficient for atomicity and correctness.

---

**58. A process can be put into a round-robin queue more than once to give it a higher priority. Running multiple instances of a program each working on a different part of a data pool can have the same effect. First write a program that tests a list of numbers for primality. Then devise a method to allow multiple instances of the program to run at once in such a way that no two instances of the program will work on the same number. Can you in fact get through the list faster by running multiple copies of the program?**  
Use a shared file or lock to assign chunks of the number list to different processes. Each program reads the next unprocessed segment, marks it as assigned, and works independently.  
Yes, **speedup is possible**, especially on multiprocessor systems or under OS time-sharing.

---

**59. Implement a program to count the frequency of words in a text file. The text file is partitioned into N segments. Each segment is processed by a separate thread that outputs the intermediate frequency count for its segment. The main process waits until all the threads complete; then it computes the consolidated word-frequency data based on the individual threads’ output.**  
Split file into N parts → spawn N threads → each thread counts words in its segment → results stored in per-thread maps → main thread merges maps. Use thread joins to wait and a final loop to sum frequency counts.

---

