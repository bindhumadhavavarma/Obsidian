## âš¡ 5.4.2 Solid State Drives (SSDs)

---

### ğŸ§  Key Characteristics
- **No moving parts** â†’ faster and more durable than magnetic disks.
- **Asymmetric performance**: Reads are typically faster than writes.
- Used widely in modern systems as primary storage devices.

---

### ğŸ“¦ SSD Interfaces

#### 1. **SATA SSDs**
- Legacy interface, originally designed for **mechanical disks**.
- Limits SSD performance due to narrow command queues and slower interface.

#### 2. **NVMe SSDs** (Non-Volatile Memory Express)
- Designed to fully exploit **PCIe** interface.
- Supports **high-speed parallel access** and **lower latency**.
- NVMe architecture is optimized for:
  - **Multi-core processors**
  - **Parallel flash chip access**

---

### ğŸ§µ NVMe Queues: Structure & Benefits

- **Multiple Queues per Core**
  - Each CPU core can have:
    - A **Submission Queue** (to send commands)
    - A **Completion Queue** (to receive results)

- **Doorbell Mechanism**
  - After filling the submission queue, the CPU core writes to a *doorbell register*.
  - This alerts the SSD controller to process pending commands.

- **Parallel Processing**
  - NVMe supports:
    - Up to **64K queues**, each with up to **64K commands**.
  - Enables SSD to handle high IOPS (Input/Output Operations Per Second).

---

### âœ… Advantages of NVMe over SATA

| Feature                     | SATA               | NVMe                        |
|----------------------------|--------------------|-----------------------------|
| Queue depth                | 1 queue, ~32 cmds   | 64K queues Ã— 64K cmds each |
| Parallelism                | Limited             | Per-core queues             |
| Latency                    | Higher              | Lower                       |
| Power efficiency           | Moderate            | Better (fewer drives needed)|
| Software layers            | More overhead       | Fewer layers (more direct access) |

- **System efficiency improves**: fewer devices needed for same throughput.
- **Reduced power & cooling needs** due to better performance per watt.
- **File systems benefit** from more direct access to storage.

---

### ğŸ§° NVMe Drivers in Operating Systems
- Most OSes (Linux, Windows, macOS) now **natively support NVMe**.
- NVMe driver stack often includes:
  - **Hardware-independent logic**
  - **PCIe communication module**
  - Optional: **NVMe over TCP or RDMA modules**

> Thanks to NVMe standardization, **a single OS driver can handle all NVMe-compliant SSDs**.

---
## ğŸ›¡ï¸ 5.4.3 Stable Storage

### ğŸ’¡ Motivation
- Disk and CPU failures can cause:
  - **Incorrect writes** (corrupted sectors)
  - **Data loss**
- For critical applications, **data must either be written correctly or not at all**.
- Goal: A disk system where **writes are either successful or leave the old value intact**.

---

### âœ… Assumptions
1. **ECC detects most write errors** â€” probability of undetected errors is extremely low (â‰ˆ 2â»Â¹â´â´).
2. **Spontaneous block decay is rare**, and *simultaneous decay on both drives* is negligible.
3. **CPU failures halt all in-progress disk writes**.
4. The system uses **two identical disks**, where each logical block is stored on both.

---

### ğŸ” Stable Storage Operations

#### 1. ğŸ“ Stable Write
- Write block to **Disk 1** â†’ verify â†’ retry if needed (up to *n* times).
- If it fails after *n* attempts â†’ remap to spare sector.
- Once Disk 1 is successful â†’ do the same on **Disk 2**.
- Ensures **both disks contain correct block** before write is considered successful.

#### 2. ğŸ“– Stable Read
- Try reading from **Disk 1**.
  - If ECC fails â†’ retry up to *n* times.
  - If still fails â†’ read from **Disk 2**.
- Based on assumption: *at least one copy is correct*.

#### 3. ğŸ”„ Crash Recovery
- After crash: scan both disks block-by-block.
  - If both are valid and equal â†’ OK.
  - If one is bad â†’ copy from the good one.
  - If both valid but unequal â†’ copy from **Disk 1 to Disk 2**.

---

### ğŸ’¥ Crash Scenarios

| Crash Point      | Result | Recovery Action |
|------------------|--------|------------------|
| (a) Before any write | Both old values remain | No action needed |
| (b) During Disk 1 write | Disk 1 corrupted, Disk 2 intact | Copy from Disk 2 to Disk 1 |
| (c) Between Disk 1 and 2 write | Disk 1 has new data | Copy from Disk 1 to Disk 2 |
| (d) During Disk 2 write | Disk 2 corrupted | Copy from Disk 1 to Disk 2 |
| (e) After both writes | Both blocks are new | No action needed |

---

### ğŸ§  Optimization Using Nonvolatile RAM (NVRAM)
- Store the **block number being written** in NVRAM before write begins.
- After successful stable write, **clear the block number** (e.g., write invalid marker like `<1`).
- During recovery:
  - Check NVRAM â†’ if valid block number exists, verify and fix both disks.

---

### ğŸ› ï¸ If NVRAM Is Not Available
- Simulate using **special fixed block** on both disks:
  - Write block number to special block on Disk 1 â†’ verify
  - Do the same on Disk 2
  - After write is complete â†’ overwrite both with an invalid number
- Downside: **Adds 8 disk I/Os** per stable write â†’ use sparingly

---

### ğŸ“… Periodic Maintenance
- Even if errors are rare:
  - Over time, both blocks could go bad.
- **Run full scan daily**:
  - Repair any inconsistencies.
  - Ensure both disks remain **identical and correct**.
---
## ğŸ§± 5.4.3 RAID â€“ Redundant Array of Independent Disks

---

### ğŸ¯ Motivation
- Disk I/O performance lagged far behind CPU performance.
- Parallelism in disk access was introduced to **improve performance and reliability**.
- RAID = Redundant Array of Inexpensive (later "Independent") Disks.
- Replacement for SLED (Single Large Expensive Disk).

---

### âš™ï¸ Basic RAID Architecture
- A RAID system contains **multiple disks**, a **RAID controller**, and appears as a **single logical disk** to the OS.
- **Data striping** is used to distribute data across disks.
- Key RAID benefits:
  - **Parallel I/O** â†’ higher throughput
  - **Redundancy** â†’ better reliability

---

### ğŸ§© RAID Levels Overview

![[Pasted image 20250414193256.png]]

#### **RAID 0 â€“ Striping (No Redundancy)**
- Data is striped across *k* disks in a round-robin pattern.
- Great for **large, sequential reads/writes**.
- No fault tolerance: **any disk failure = complete data loss**.
- âš ï¸ Not a "true" RAID (no redundancy).

#### **RAID 1 â€“ Mirroring**
- All data is **duplicated** across mirrored disks.
- Write performance = single drive.
- Read performance: **up to 2Ã— better** due to load distribution.
- **Excellent fault tolerance**; easy recovery by copying from backup.

#### **RAID 2 â€“ Bit-Level Striping with Hamming Code**
- Each bit of data + parity bits (e.g., Hamming) are spread over separate disks.
- Disks must be **synchronized**.
- Used in special systems (e.g., Thinking Machines CM-2).
- High throughput but **high controller complexity** and overhead.

#### **RAID 3 â€“ Byte/Word Striping with Single Parity Disk**
- Data striped at byte/word level; single dedicated **parity disk**.
- Can **recover from single disk failure** by XORing parity with others.
- Requires disk synchronization.
- High bandwidth, but **poor IOPS** (I/O operations per second).

#### **RAID 4 â€“ Block-Level Striping with Dedicated Parity**
- Like RAID 0, but with an **extra parity disk**.
- On write:
  - Read old data & parity
  - Update parity
  - Write data + parity
- **Parity disk is a bottleneck** for small writes.

#### **RAID 5 â€“ Block-Level Striping with Distributed Parity**
- Distributes both **data and parity** across all disks.
- Avoids parity bottleneck of RAID 4.
- More complex rebuilds on failure.

#### **RAID 6 â€“ Dual Parity**
- Like RAID 5 but with **two parity blocks** per stripe.
- Can tolerate **2 simultaneous disk failures**.
- Read performance = RAID 5
- Write performance = slightly worse (more parity calculations).
- Suitable for higher-reliability use cases.

---

### âš¡ RAID + SSDs

- SSDs are fast and reliable, but **RAID still useful**:
  - **RAID 0** over SSDs: excellent sequential throughput.
  - **RAID 1**: improves read reliability (and sometimes speed).
  - **RAID 5/6**:
    - More resilient, **but write-heavy**.
    - SSDs suffer more from frequent writes (wear-leveling, endurance).

---

### âš ï¸ RAID Tradeoffs Summary

| RAID Level | Striping | Redundancy | Fault Tolerance | Read | Write | Cost | Notes |
|------------|----------|------------|------------------|------|-------|------|-------|
| RAID 0     | âœ…        | âŒ         | âŒ               | ğŸ”¼   | ğŸ”¼    | ğŸ’²   | Fast but risky |
| RAID 1     | âŒ        | âœ… (mirror)| âœ… (1 disk)       | ğŸ”¼   | â–    | ğŸ’°ğŸ’° | Very reliable |
| RAID 2     | Bit-level| ECC        | âœ… (1 bit)        | ğŸ”¼   | ğŸ”½    | ğŸ’°ğŸ’°ğŸ’° | Obsolete, high overhead |
| RAID 3     | Byte/word| Parity     | âœ… (1 disk)       | ğŸ”¼   | â–    | ğŸ’°ğŸ’° | Synchronized drives |
| RAID 4     | Block    | Parity     | âœ… (1 disk)       | ğŸ”¼   | ğŸ”½    | ğŸ’°ğŸ’° | Parity bottleneck |
| RAID 5     | Block    | Dist. Parity| âœ… (1 disk)       | ğŸ”¼   | â–    | ğŸ’°ğŸ’° | Balanced |
| RAID 6     | Block    | Dual Parity| âœ…âœ… (2 disks)     | ğŸ”¼   | ğŸ”½    | ğŸ’°ğŸ’°ğŸ’° | Highest reliability |

---

