### Message Passing

- Message passing is an interprocess communication mechanism using two system calls:
  - `send(destination, &message);`
  - `receive(source, &message);`
- Unlike monitors (which are language constructs), message passing is implemented as system calls and can easily be placed in libraries.

#### Design Issues in Message-Passing Systems

- **Reliability**:
  - Messages may get **lost**, especially over networks.
  - Acknowledgements can help ensure delivery.
  - If an ACK is lost, sender may **retransmit**, leading to **duplicate messages**.
  - Solution: **Sequence numbers** to detect duplicates.

- **Naming**:
  - Processes must be uniquely identified in send/receive.
  
- **Authentication**:
  - Ensure sender is communicating with the correct process (e.g., real file server, not imposter).

- **Performance**:
  - Copying messages is **slower** than semaphores/monitors.
  - Optimization is crucial, especially for same-machine communication.

---

### Producer-Consumer with Message Passing

- No shared memory is used.
- Assumes messages are fixed-size and OS buffers undelivered messages.
- Total of `N` messages used, just like N buffer slots.
- Initial setup:
  - **Consumer sends N empty messages** to producer.
  - **Producer** receives an empty, sends back a **full** message with data.

#### Behavior:

- If **producer is faster**, all messages become full and producer blocks waiting for an empty.
- If **consumer is faster**, all messages are empty and consumer blocks waiting for a full.

---

### Variants in Message Addressing

- **Direct addressing**:
  - Each process has a unique address.
  - Messages are sent directly to process IDs.
  
- **Mailboxes**:
  - Each mailbox can buffer `N` messages.
  - `send`/`receive` use mailbox addresses, not process IDs.
  - If mailbox is **full**, sender blocks.
  - Producer sends to consumer's mailbox; consumer sends empty messages to producer’s mailbox.

- **No buffering (Rendezvous)**:
  - `send` blocks until `receive` is issued and vice versa.
  - Sender and receiver must run in **lockstep**.
  - Easier to implement, but **less flexible**.

---

### Code Example: Producer-Consumer with Message Passing

```c
#define N 100 /* number of slots in the buffer */

void producer(void) {
  int item;
  message m; /* message buffer */
  while (TRUE) {
    item = produce_item();          /* generate something to put in buffer */
    receive(consumer, &m);          /* wait for an empty to arrive */
    build_message(&m, item);        /* construct a message to send */
    send(consumer, &m);             /* send item to consumer */
  }
}

void consumer(void) {
  int item, i;
  message m;
  for (i = 0; i < N; i++) send(producer, &m); /* send N empties */
  while (TRUE) {
    receive(producer, &m);          /* get message containing item */
    item = extract_item(&m);        /* extract item from message */
    send(producer, &m);             /* send back empty reply */
    consume_item(item);             /* do something with the item */
  }
}
```

---

### Applications

- Common in **parallel programming** systems.
- Widely used in **distributed systems** and **scientific computing**.
- **MPI (Message-Passing Interface)** is a popular system for high-performance computing.


### 2.4.9 Barriers

- **Barriers** are synchronization mechanisms used when multiple processes must wait for each other at certain points (end of a phase) before proceeding.
- A barrier blocks all processes until *every* participating process reaches the barrier.
- Once all are at the barrier, they are released together.

#### Example Use Case
- In iterative scientific computations (e.g., heat distribution on a metal sheet), different processes calculate parts of a matrix in phases.
- A barrier at the end of each phase ensures no process starts phase *n+1* before all have finished phase *n*.

#### Memory Barriers / Memory Fences
- Enforce ordering of memory operations (reads/writes) on modern CPUs that execute instructions out of order.
- Prevent issues in concurrent programs where reordering can lead to inconsistent states.

**Example:**

```c
// THREAD 1
while (turn != 1) { } // loop
printf("%d\n", x);

// THREAD 2
x = 100;
turn = 1;
```

- If instructions are reordered, `turn` might be updated before `x`, causing incorrect output.
- A memory barrier ensures proper instruction execution order.

#### Relevance
- Memory barriers are used to mitigate **transient execution vulnerabilities** like **Meltdown** and **Spectre** where CPUs speculatively execute instructions.

---

### 2.4.10 Priority Inversion

- **Priority inversion** occurs when a low-priority thread holds a lock needed by a high-priority thread, but gets preempted by a medium-priority thread.
- This blocks the high-priority thread indefinitely.

#### Real-World Example
- **Mars Pathfinder (1997)** had this issue:
  - Low-priority meteorological thread held a mutex.
  - Medium-priority communication thread preempted it.
  - High-priority system thread got blocked waiting for the mutex.
  - System resets were needed due to thread deadlock.

#### Solutions
1. **Disable interrupts** (not ideal for user programs).
2. **Priority Ceiling**: Boosts the thread's priority to the highest level any waiting thread may have.
3. **Priority Inheritance**: Temporarily raises the priority of the thread holding the lock.
   - Used to fix Mars Pathfinder issue.
4. **Random Boosting**: OS occasionally boosts priorities of mutex-holding threads.

---

### 2.4.11 Avoiding Locks: Read-Copy-Update (RCU)

- **RCU** is a lockless synchronization technique that allows concurrent read and write access to data structures.

#### Concept
- Writer creates a new version of the data structure and then makes it visible to readers in one atomic step.
- Readers either see the *old* or *new* version—not a mix.
- Removal and reclamation are decoupled:
  - Nodes are only removed from memory after ensuring no readers are accessing them.

#### Key Ideas
- Readers operate in **read-side critical sections** (cannot block or sleep).
- A **grace period** is defined where all threads must have exited their read sections.
- Memory reclamation happens only after this period to ensure safety.

#### Example (Tree Update):
1. Add node `X` under `A`:
   - Initialize `X`, attach it under `E`, then atomically update `A` to point to `X`.
   - Readers in `A` see new tree; others still see the old tree.
2. Remove nodes `B` and `D`:
   - Point `A`'s child to `C`, removing `B` and `D` from the path.
   - Wait for readers in `B/D` to finish, then safely delete.

#### Usage
- Common in kernel-level code (e.g., Linux kernel) where concurrency and performance are critical.
- Found in networking, file systems, memory management, etc.

RCU is an efficient alternative to locks in read-heavy scenarios with occasional writes.