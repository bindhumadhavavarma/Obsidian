
## 🌀 4.3.5 Log-Structured File Systems (LFS)

### 📈 Motivation
- **Disks are growing** in size and **CPUs/RAMs are faster**, but **seek times are stagnant**, causing:
  - Performance bottlenecks.
  - Inefficiencies in **frequent small writes**.
- Most reads are now served from **disk cache**, so **writes dominate disk activity**.

### 🔄 Core Idea
- Treat the **entire disk as a circular log**.
- **All file system writes** (data blocks, i-nodes, directories) are batched into **large segments** (e.g., 1 MB) and written sequentially.
- ✅ Achieves **full disk bandwidth**, even with small writes.

### 📌 Key Concepts

| Component         | Description |
|------------------|-------------|
| **Segment**       | Large, contiguous block written to disk (contains data, i-nodes, directory entries). |
| **Segment summary** | Stored at start of each segment; lists contents. |
| **I-node map**     | Maps i-node number → location in log. Cached for speed. |
| **Cleaner thread** | Scans old segments to reclaim space by removing stale data. |

### 🔁 How It Works
1. Writes accumulate in memory.
2. Periodically flushed to disk as a segment.
3. i-nodes are updated and logged.
4. Cleaner reclaims unused segments by:
   - Reading segment summaries.
   - Checking liveness using the i-node map.
   - Rewriting live data into new segments.
   - Marking old segments free.

### ⚙️ Benefits
- 🚀 Fast small writes.
- 👌 Equal or better performance for reads and large writes.
- 🧠 Intelligent disk usage via cleaner and segment structure.

### ⚠️ Challenges
- Managing i-node updates and maintaining consistency.
- Complexity in **segment cleaning** and **i-node remapping**.

---

## 🛡️ 4.3.6 Journaling File Systems

### 🔧 Problem Addressed
- File system **inconsistencies due to crashes**.
- Example: Crashing during file deletion can cause:
  - **Resource leakage** (i-node or blocks lost).
  - **Corrupted pointers** (directory points to wrong i-node).
  - **Block reuse conflicts** (multiple files using same blocks).

### 🧾 Journaling Concept
- Maintain a **log of upcoming file system operations** (like a to-do list).
- If a crash occurs, the log is consulted and pending operations are **completed or rolled back**.

### 🔁 Typical Workflow
1. Log planned actions to disk (e.g., delete file steps).
2. Execute operations.
3. If all succeed → erase log entry.
4. On reboot → system checks log, retries incomplete operations.

### 💡 Idempotency Requirement
- All operations in journal must be **idempotent**:
  - Can be safely **repeated multiple times** without changing the outcome.
- Examples:
  - ✅ `Mark block n as free`
  - ✅ `Remove file "foo" from directory`
  - ❌ `Append block to end of free list` (must be checked first)

### 🧩 Atomic Transactions
- Operations can be bracketed with `begin_transaction` and `end_transaction`.
- Ensures **all-or-nothing execution** of grouped operations.

---

### 📦 Real-World Examples

| File System | Journal Support | Notes |
|-------------|------------------|-------|
| **NTFS**     | ✅ Full journaling | Stable since 1993 |
| **ext3**     | ✅ Compatible with ext2 | Conservative, safe |
| **ext4**     | ✅ Backward-compatible | Faster and scalable |
| **ReiserFS** | ✅ Ambitious journaling | Initially incompatible with ext2 |
| **MacOS**    | ✅ Optional journaling | Enabled by default in recent versions |

---
## ⚡ 4.3.7 Flash-Based File Systems

### 🧠 Key Differences: Flash vs. Magnetic Disks
- ❌ No moving parts: eliminates **seek time** and **rotational delays**.
- ✅ **Random & sequential reads** nearly equivalent in speed.
- ❌ **Writes are slower** and **more complex** due to:
  - **Erase-before-write** requirement.
  - **Asymmetric performance**: reads (μs) ≪ writes (100s of μs).
  - Write must occur **sequentially** within a flash block.
  - Cannot overwrite flash pages directly.

---

### 📏 Flash Storage Terminology

| Term          | Description |
|---------------|-------------|
| **Flash page** | Smallest I/O unit (typically 4KB) |
| **Flash block** | Erase unit, contains multiple pages (often MBs) |
| **P/E cycle** | Program/Erase cycle – limited lifespan (few k to 100k cycles) |

---

### 🧪 Flash Translation Layer (FTL)

- A **firmware layer** in SSDs handling:
  - Wear leveling
  - Logical ↔ physical address translation
  - Garbage collection

- Works like **virtual memory paging**:
  - E.g., Logical Block 54321 → Physical: FP1 > die0 > plane2 > block5 > page3

---

### 🗑️ Garbage Collection (GC)

- **Purpose**: reclaim space from flash blocks with invalid pages.
- **Steps**:
  1. Select **victim block** (with few valid pages).
  2. Copy valid pages to a **target block**.
  3. Erase the original block.

- GC can interfere with I/O or trigger **write amplification**:
  - Multiple valid pages may need to be copied even for a small update.
  - Especially bad with **random writes** and **block-level mappings**.

---

### ♻️ Wear-Leveling Strategies
- **Avoid overusing certain blocks**.
- Prefer:
  - Spreading writes
  - Grouping **hot** (frequently updated) and **cold** data.

---

### 📉 Semantic Gap: File System vs. SSD
- File systems **don’t know** what’s invalid inside SSD unless they **explicitly inform it**.
- **TRIM command**: Notifies SSD which pages are safe to erase.
  - Without TRIM: SSD only detects invalid pages **after overwrite attempts**.

---

### 🚧 Challenges with Traditional File Systems (e.g., ext4, NTFS)

| Issue | Description |
|-------|-------------|
| **Inefficient writes** | Frequent updates to metadata cause multiple writes |
| **Recursive updates** | One file update → indirect block update → i-node → i-node map |
| **Mapping granularity** | Page vs block mapping tradeoffs affect speed and space |

---

### 🔁 Why Log-Structured File Systems (LFS) Suit Flash

| Benefit | Why it matches flash |
|--------|----------------------|
| **Sequential writes** | Flash prefers writes to fresh blocks |
| **Avoids overwrites** | Flash pages can't be modified in-place |
| **Write once model** | Aligns well with flash erase constraints |

---

### 🧩 Recursive Update Problem (Wandering Tree)
- Updating data → indirect block → i-node → i-node map
- Causes multiple writes per user action.

**🛠️ Solution**: Use **fixed logical locations** for mappings (as in **F2FS**):
- Store only i-node/indirect block **IDs**, not addresses.
- Global map resolves these IDs to physical addresses.

---

### 🌟 Flash-Aware File Systems

| File System | Highlights |
|-------------|------------|
| **F2FS** (Flash-Friendly FS) | Optimized LFS variant, reduces recursive updates, Linux kernel-supported |
| **YAFFS**, **JFFS2** | Earlier embedded NAND file systems |
| **UBIFS** | Scalable flash FS for large storage |

---

### 🧠 Recap: Why Flash Needs Custom FS
- **High read performance**, but **complex writes**.
- **Wear sensitivity** requires careful data placement.
- File system and SSD must **collaborate** (e.g., via TRIM).
- Flash is **not** a simple HDD replacement; it demands **architectural rethinking**.

---
## 📁 4.3.8 Virtual File Systems (VFS)

### 🧩 Motivation
- Multiple file systems often coexist:
  - 🪟 **Windows** uses drive letters: `C:`, `D:` for FAT, NTFS, USB, etc.
  - 🐧 **UNIX/Linux** integrates all FS into a **single hierarchy**.
- E.g., `/usr` can be `ext3`, `/home` on ReiserFS, `/mnt` on F2FS — all look seamless to users.

---

### 📐 VFS Architecture
- VFS **abstracts common file system operations** and provides:
  - **Upper interface**: standard POSIX calls (open, read, write, lseek…).
  - **Lower interface**: function calls to specific (concrete) file systems.

#### 📊 Diagram Overview
```
+---------------------+
|     User Process    |
+---------------------+
          |
          v
+---------------------+
|  Virtual File System| <-- POSIX syscalls
+---------------------+
          |
          v
+---------------------+
| FS1 | FS2 | FS3 ... | <-- Each FS provides function vector
+---------------------+
```

---

### ⚙️ VFS Key Components

| Component      | Description |
|----------------|-------------|
| **Superblock** | Describes a file system (metadata) |
| **V-node**     | In-memory structure for an open file |
| **Directory**  | Represents a directory object |
| **Mount Table**| Lists all currently mounted file systems |
| **File Descriptors** | Maps open files to v-nodes for each process |

- Though VFS is implemented in C, it follows an **object-oriented** model.

---

### 🧵 Example: File Access via VFS

1. System boots → root file system **registers** with VFS.
2. Other FS (e.g., ext3 on `/usr`) **mounted and registered** too.
3. Process opens:  
   `open("/usr/include/unistd.h", O_RDONLY)`
   - VFS locates `/usr`, finds it’s a mount point.
   - Locates `/usr`’s superblock, then follows path to `unistd.h`.
   - Creates a **v-node**, copies i-node data, links function table.
   - Updates file descriptor table to point to this v-node.
   - Returns file descriptor to process.

4. On `read(fd)`:
   - VFS fetches v-node via fd.
   - Follows function pointer → calls **read handler** in correct FS.
   - Actual block is retrieved by the underlying FS (local or remote).

#### 📌 Key Insight
> VFS doesn't know/care *how or where* the data is stored — just that the FS provides the required functions.

---

### 🧑‍💻 Porting or Adding a New FS

- FS must **register a function table** (e.g., open, read, write, etc.).
- New FS can be:
  - **Custom-written** with VFS interface in mind.
  - **Wrapped/ported** if it’s pre-existing (create adapters/wrappers).

---

### 🖼️ Data Structures in Action

```mermaid
graph LR
A[User Process] --> B[File Descriptor Table]
B --> C[V-node Table]
C --> D[Function Pointer Table (VFS)]
D --> E[Read function in FS1]
```

> Used for all I/O operations after file is opened.

---

### 🌍 Bonus: Supporting Remote File Systems
- VFS was originally designed (by Sun) to support **NFS**.
- As long as a FS implements required functions, it can be:
  - Local disk
  - USB
  - Remote over network
  - Even encrypted or virtual file systems

---

