
## ğŸŒ€ 4.3.5 Log-Structured File Systems (LFS)

### ğŸ“ˆ Motivation
- **Disks are growing** in size and **CPUs/RAMs are faster**, but **seek times are stagnant**, causing:
  - Performance bottlenecks.
  - Inefficiencies in **frequent small writes**.
- Most reads are now served from **disk cache**, so **writes dominate disk activity**.

### ğŸ”„ Core Idea
- Treat the **entire disk as a circular log**.
- **All file system writes** (data blocks, i-nodes, directories) are batched into **large segments** (e.g., 1 MB) and written sequentially.
- âœ… Achieves **full disk bandwidth**, even with small writes.

### ğŸ“Œ Key Concepts

| Component         | Description |
|------------------|-------------|
| **Segment**       | Large, contiguous block written to disk (contains data, i-nodes, directory entries). |
| **Segment summary** | Stored at start of each segment; lists contents. |
| **I-node map**     | Maps i-node number â†’ location in log. Cached for speed. |
| **Cleaner thread** | Scans old segments to reclaim space by removing stale data. |

### ğŸ” How It Works
1. Writes accumulate in memory.
2. Periodically flushed to disk as a segment.
3. i-nodes are updated and logged.
4. Cleaner reclaims unused segments by:
   - Reading segment summaries.
   - Checking liveness using the i-node map.
   - Rewriting live data into new segments.
   - Marking old segments free.

### âš™ï¸ Benefits
- ğŸš€ Fast small writes.
- ğŸ‘Œ Equal or better performance for reads and large writes.
- ğŸ§  Intelligent disk usage via cleaner and segment structure.

### âš ï¸ Challenges
- Managing i-node updates and maintaining consistency.
- Complexity in **segment cleaning** and **i-node remapping**.

---

## ğŸ›¡ï¸ 4.3.6 Journaling File Systems

### ğŸ”§ Problem Addressed
- File system **inconsistencies due to crashes**.
- Example: Crashing during file deletion can cause:
  - **Resource leakage** (i-node or blocks lost).
  - **Corrupted pointers** (directory points to wrong i-node).
  - **Block reuse conflicts** (multiple files using same blocks).

### ğŸ§¾ Journaling Concept
- Maintain a **log of upcoming file system operations** (like a to-do list).
- If a crash occurs, the log is consulted and pending operations are **completed or rolled back**.

### ğŸ” Typical Workflow
1. Log planned actions to disk (e.g., delete file steps).
2. Execute operations.
3. If all succeed â†’ erase log entry.
4. On reboot â†’ system checks log, retries incomplete operations.

### ğŸ’¡ Idempotency Requirement
- All operations in journal must be **idempotent**:
  - Can be safely **repeated multiple times** without changing the outcome.
- Examples:
  - âœ… `Mark block n as free`
  - âœ… `Remove file "foo" from directory`
  - âŒ `Append block to end of free list` (must be checked first)

### ğŸ§© Atomic Transactions
- Operations can be bracketed with `begin_transaction` and `end_transaction`.
- Ensures **all-or-nothing execution** of grouped operations.

---

### ğŸ“¦ Real-World Examples

| File System | Journal Support | Notes |
|-------------|------------------|-------|
| **NTFS**     | âœ… Full journaling | Stable since 1993 |
| **ext3**     | âœ… Compatible with ext2 | Conservative, safe |
| **ext4**     | âœ… Backward-compatible | Faster and scalable |
| **ReiserFS** | âœ… Ambitious journaling | Initially incompatible with ext2 |
| **MacOS**    | âœ… Optional journaling | Enabled by default in recent versions |

---
## âš¡ 4.3.7 Flash-Based File Systems

### ğŸ§  Key Differences: Flash vs. Magnetic Disks
- âŒ No moving parts: eliminates **seek time** and **rotational delays**.
- âœ… **Random & sequential reads** nearly equivalent in speed.
- âŒ **Writes are slower** and **more complex** due to:
  - **Erase-before-write** requirement.
  - **Asymmetric performance**: reads (Î¼s) â‰ª writes (100s of Î¼s).
  - Write must occur **sequentially** within a flash block.
  - Cannot overwrite flash pages directly.

---

### ğŸ“ Flash Storage Terminology

| Term          | Description |
|---------------|-------------|
| **Flash page** | Smallest I/O unit (typically 4KB) |
| **Flash block** | Erase unit, contains multiple pages (often MBs) |
| **P/E cycle** | Program/Erase cycle â€“ limited lifespan (few k to 100k cycles) |

---

### ğŸ§ª Flash Translation Layer (FTL)

- A **firmware layer** in SSDs handling:
  - Wear leveling
  - Logical â†” physical address translation
  - Garbage collection

- Works like **virtual memory paging**:
  - E.g., Logical Block 54321 â†’ Physical: FP1 > die0 > plane2 > block5 > page3

---

### ğŸ—‘ï¸ Garbage Collection (GC)

- **Purpose**: reclaim space from flash blocks with invalid pages.
- **Steps**:
  1. Select **victim block** (with few valid pages).
  2. Copy valid pages to a **target block**.
  3. Erase the original block.

- GC can interfere with I/O or trigger **write amplification**:
  - Multiple valid pages may need to be copied even for a small update.
  - Especially bad with **random writes** and **block-level mappings**.

---

### â™»ï¸ Wear-Leveling Strategies
- **Avoid overusing certain blocks**.
- Prefer:
  - Spreading writes
  - Grouping **hot** (frequently updated) and **cold** data.

---

### ğŸ“‰ Semantic Gap: File System vs. SSD
- File systems **donâ€™t know** whatâ€™s invalid inside SSD unless they **explicitly inform it**.
- **TRIM command**: Notifies SSD which pages are safe to erase.
  - Without TRIM: SSD only detects invalid pages **after overwrite attempts**.

---

### ğŸš§ Challenges with Traditional File Systems (e.g., ext4, NTFS)

| Issue | Description |
|-------|-------------|
| **Inefficient writes** | Frequent updates to metadata cause multiple writes |
| **Recursive updates** | One file update â†’ indirect block update â†’ i-node â†’ i-node map |
| **Mapping granularity** | Page vs block mapping tradeoffs affect speed and space |

---

### ğŸ” Why Log-Structured File Systems (LFS) Suit Flash

| Benefit | Why it matches flash |
|--------|----------------------|
| **Sequential writes** | Flash prefers writes to fresh blocks |
| **Avoids overwrites** | Flash pages can't be modified in-place |
| **Write once model** | Aligns well with flash erase constraints |

---

### ğŸ§© Recursive Update Problem (Wandering Tree)
- Updating data â†’ indirect block â†’ i-node â†’ i-node map
- Causes multiple writes per user action.

**ğŸ› ï¸ Solution**: Use **fixed logical locations** for mappings (as in **F2FS**):
- Store only i-node/indirect block **IDs**, not addresses.
- Global map resolves these IDs to physical addresses.

---

### ğŸŒŸ Flash-Aware File Systems

| File System | Highlights |
|-------------|------------|
| **F2FS** (Flash-Friendly FS) | Optimized LFS variant, reduces recursive updates, Linux kernel-supported |
| **YAFFS**, **JFFS2** | Earlier embedded NAND file systems |
| **UBIFS** | Scalable flash FS for large storage |

---

### ğŸ§  Recap: Why Flash Needs Custom FS
- **High read performance**, but **complex writes**.
- **Wear sensitivity** requires careful data placement.
- File system and SSD must **collaborate** (e.g., via TRIM).
- Flash is **not** a simple HDD replacement; it demands **architectural rethinking**.

---
## ğŸ“ 4.3.8 Virtual File Systems (VFS)

### ğŸ§© Motivation
- Multiple file systems often coexist:
  - ğŸªŸ **Windows** uses drive letters: `C:`, `D:` for FAT, NTFS, USB, etc.
  - ğŸ§ **UNIX/Linux** integrates all FS into a **single hierarchy**.
- E.g., `/usr` can be `ext3`, `/home` on ReiserFS, `/mnt` on F2FS â€” all look seamless to users.

---

### ğŸ“ VFS Architecture
- VFS **abstracts common file system operations** and provides:
  - **Upper interface**: standard POSIX calls (open, read, write, lseekâ€¦).
  - **Lower interface**: function calls to specific (concrete) file systems.

#### ğŸ“Š Diagram Overview
```
+---------------------+
|     User Process    |
+---------------------+
          |
          v
+---------------------+
|  Virtual File System| <-- POSIX syscalls
+---------------------+
          |
          v
+---------------------+
| FS1 | FS2 | FS3 ... | <-- Each FS provides function vector
+---------------------+
```

---

### âš™ï¸ VFS Key Components

| Component      | Description |
|----------------|-------------|
| **Superblock** | Describes a file system (metadata) |
| **V-node**     | In-memory structure for an open file |
| **Directory**  | Represents a directory object |
| **Mount Table**| Lists all currently mounted file systems |
| **File Descriptors** | Maps open files to v-nodes for each process |

- Though VFS is implemented in C, it follows an **object-oriented** model.

---

### ğŸ§µ Example: File Access via VFS

1. System boots â†’ root file system **registers** with VFS.
2. Other FS (e.g., ext3 on `/usr`) **mounted and registered** too.
3. Process opens:  
   `open("/usr/include/unistd.h", O_RDONLY)`
   - VFS locates `/usr`, finds itâ€™s a mount point.
   - Locates `/usr`â€™s superblock, then follows path to `unistd.h`.
   - Creates a **v-node**, copies i-node data, links function table.
   - Updates file descriptor table to point to this v-node.
   - Returns file descriptor to process.

4. On `read(fd)`:
   - VFS fetches v-node via fd.
   - Follows function pointer â†’ calls **read handler** in correct FS.
   - Actual block is retrieved by the underlying FS (local or remote).

#### ğŸ“Œ Key Insight
> VFS doesn't know/care *how or where* the data is stored â€” just that the FS provides the required functions.

---

### ğŸ§‘â€ğŸ’» Porting or Adding a New FS

- FS must **register a function table** (e.g., open, read, write, etc.).
- New FS can be:
  - **Custom-written** with VFS interface in mind.
  - **Wrapped/ported** if itâ€™s pre-existing (create adapters/wrappers).

---

### ğŸ–¼ï¸ Data Structures in Action

```mermaid
graph LR
A[User Process] --> B[File Descriptor Table]
B --> C[V-node Table]
C --> D[Function Pointer Table (VFS)]
D --> E[Read function in FS1]
```

> Used for all I/O operations after file is opened.

---

### ğŸŒ Bonus: Supporting Remote File Systems
- VFS was originally designed (by Sun) to support **NFS**.
- As long as a FS implements required functions, it can be:
  - Local disk
  - USB
  - Remote over network
  - Even encrypted or virtual file systems

---

